{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiments.utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils\n",
    "> Set of functions to easily perform experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ENV_VARS = dict(OMP_NUM_THREADS='2',\n",
    "                OPENBLAS_NUM_THREADS='2',\n",
    "                MKL_NUM_THREADS='3',\n",
    "                VECLIB_MAXIMUM_THREADS='2',\n",
    "                NUMEXPR_NUM_THREADS='3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "# Limit number of threads in numpy and others to avoid throttling\n",
    "os.environ.update(ENV_VARS)\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch as t\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from nixtlats.data.scalers import Scaler\n",
    "from nixtlats.data.tsdataset import TimeSeriesDataset, WindowsDataset\n",
    "from nixtlats.data.tsloader import TimeSeriesLoader\n",
    "from nixtlats.models.esrnn.esrnn import ESRNN\n",
    "from nixtlats.models.esrnn.mqesrnn import MQESRNN\n",
    "from nixtlats.models.nbeats.nbeats import NBEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_mask_dfs(Y_df, ds_in_val, ds_in_test):\n",
    "    # train mask\n",
    "    train_mask_df = Y_df.copy()[['unique_id', 'ds']]\n",
    "    train_mask_df.sort_values(by=['unique_id', 'ds'], inplace=True)\n",
    "    train_mask_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_mask_df['sample_mask'] = 1\n",
    "    train_mask_df['available_mask'] = 1\n",
    "    \n",
    "    idx_out = train_mask_df.groupby('unique_id').tail(ds_in_val+ds_in_test).index\n",
    "    train_mask_df.loc[idx_out, 'sample_mask'] = 0\n",
    "    \n",
    "    # test mask\n",
    "    test_mask_df = train_mask_df.copy()\n",
    "    test_mask_df['sample_mask'] = 0\n",
    "    idx_test = test_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    test_mask_df.loc[idx_test, 'sample_mask'] = 1\n",
    "    \n",
    "    # validation mask\n",
    "    val_mask_df = train_mask_df.copy()\n",
    "    val_mask_df['sample_mask'] = 1\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - train_mask_df['sample_mask']\n",
    "    val_mask_df['sample_mask'] = val_mask_df['sample_mask'] - test_mask_df['sample_mask']\n",
    "\n",
    "    assert len(train_mask_df)==len(Y_df), \\\n",
    "        f'The mask_df length {len(train_mask_df)} is not equal to Y_df length {len(Y_df)}'\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_random_mask_dfs(Y_df, ds_in_test, \n",
    "                        n_val_windows, n_ds_val_window,\n",
    "                        n_uids, freq):\n",
    "    \"\"\"\n",
    "    Generates train, test and random validation mask.\n",
    "    Train mask begins by avoiding ds_in_test\n",
    "    \n",
    "    Validation mask: 1) samples n_uids unique ids\n",
    "                     2) creates windows of size n_ds_val_window\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_in_test: int\n",
    "        Number of ds in test.\n",
    "    n_uids: int\n",
    "        Number of unique ids in validation.\n",
    "    n_val_windows: int\n",
    "        Number of windows for validation.\n",
    "    n_ds_val_window: int\n",
    "        Number of ds in each validation window.\n",
    "    periods: int  \n",
    "        ds_in_test multiplier.\n",
    "    freq: str\n",
    "        string that determines datestamp frequency, used in\n",
    "        random windows creation.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #----------------------- Train mask -----------------------#\n",
    "    # Initialize masks\n",
    "    train_mask_df, val_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                            ds_in_val=0,\n",
    "                                                            ds_in_test=ds_in_test)\n",
    "    \n",
    "    assert val_mask_df['sample_mask'].sum()==0, 'Muerte'\n",
    "    \n",
    "    #----------------- Random Validation mask -----------------#\n",
    "    # Overwrite validation with random windows\n",
    "    uids = train_mask_df['unique_id'].unique()\n",
    "    val_uids = np.random.choice(uids, n_uids, replace=False)\n",
    "    \n",
    "    # Validation avoids test\n",
    "    idx_test = train_mask_df.groupby('unique_id').tail(ds_in_test).index\n",
    "    available_ds = train_mask_df.loc[~train_mask_df.index.isin(idx_test)]['ds'].unique()\n",
    "    val_init_ds = np.random.choice(available_ds, n_val_windows, replace=False)\n",
    "    \n",
    "    # Creates windows \n",
    "    val_ds = [pd.date_range(init, periods=n_ds_val_window, freq=freq) for init in val_init_ds]\n",
    "    val_ds = np.concatenate(val_ds)\n",
    "\n",
    "    # Cleans random windows from train mask\n",
    "    val_idx = train_mask_df.query('unique_id in @val_uids & ds in @val_ds').index\n",
    "    train_mask_df.loc[val_idx, 'sample_mask'] = 0\n",
    "    val_mask_df.loc[val_idx, 'sample_mask'] = 1\n",
    "    \n",
    "    return train_mask_df, val_mask_df, test_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scale_data(Y_df, X_df, mask_df, normalizer_y, normalizer_x):\n",
    "    mask = mask_df['available_mask'].values * mask_df['sample_mask'].values\n",
    "    \n",
    "    if normalizer_y is not None:\n",
    "        scaler_y = Scaler(normalizer=normalizer_y)\n",
    "        Y_df['y'] = scaler_y.scale(x=Y_df['y'].values, mask=mask)\n",
    "    else:\n",
    "        scaler_y = None\n",
    "\n",
    "    if normalizer_x is not None:\n",
    "        X_cols = [col for col in X_df.columns if col not in ['unique_id','ds']]\n",
    "        for col in X_cols:\n",
    "            scaler_x = Scaler(normalizer=normalizer_x)\n",
    "            X_df[col] = scaler_x.scale(x=X_df[col].values, mask=mask)\n",
    "\n",
    "    return Y_df, X_df, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_datasets(mc, S_df, Y_df, X_df, f_cols,\n",
    "                    ds_in_test, ds_in_val,\n",
    "                    n_uids, n_val_windows, freq,\n",
    "                    is_val_random):\n",
    "    #------------------------------------- Available and Validation Mask ------------------------------------#\n",
    "    if is_val_random:\n",
    "        train_mask_df, valid_mask_df, test_mask_df = get_random_mask_dfs(Y_df=Y_df, \n",
    "                                                                         ds_in_test=ds_in_test,\n",
    "                                                                         n_uids=n_uids, \n",
    "                                                                         n_val_windows=n_val_windows, \n",
    "                                                                         n_ds_val_window=ds_in_val//n_val_windows,\n",
    "                                                                         freq=freq)\n",
    "    else:\n",
    "        train_mask_df, valid_mask_df, test_mask_df = get_mask_dfs(Y_df=Y_df,\n",
    "                                                                  ds_in_test=ds_in_test, \n",
    "                                                                  ds_in_val=ds_in_val)\n",
    "\n",
    "    #---------------------------------------------- Scale Data ----------------------------------------------#\n",
    "    Y_df, X_df, scaler_y = scale_data(Y_df=Y_df, X_df=X_df, mask_df=train_mask_df,\n",
    "                                      normalizer_y=mc['normalizer_y'], normalizer_x=mc['normalizer_x'])\n",
    "\n",
    "    #----------------------------------------- Declare Dataset and Loaders ----------------------------------#\n",
    "    \n",
    "    if mc['mode'] == 'simple':\n",
    "        train_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = WindowsDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      sample_freq=int(mc['val_idx_to_sample_freq']),\n",
    "                                      verbose=True)\n",
    "    \n",
    "    if mc['mode'] == 'full':\n",
    "        train_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=train_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        valid_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                       mask_df=valid_mask_df, f_cols=f_cols,\n",
    "                                       input_size=int(mc['n_time_in']),\n",
    "                                       output_size=int(mc['n_time_out']),\n",
    "                                       verbose=True)\n",
    "        \n",
    "        test_dataset = TimeSeriesDataset(S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                      mask_df=test_mask_df, f_cols=f_cols,\n",
    "                                      input_size=int(mc['n_time_in']),\n",
    "                                      output_size=int(mc['n_time_out']),\n",
    "                                      verbose=True)        \n",
    "    \n",
    "    if ds_in_test == 0:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_loaders(mc, train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = TimeSeriesLoader(dataset=train_dataset,\n",
    "                                    batch_size=int(mc['batch_size']),\n",
    "                                    eq_batch_size=mc.get('eq_batch_size') or True,\n",
    "                                    shuffle=True)\n",
    "    if val_dataset is not None:\n",
    "        val_loader = TimeSeriesLoader(dataset=val_dataset,\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False)\n",
    "        \n",
    "    else:\n",
    "        val_loader = None\n",
    "\n",
    "    if test_dataset is not None:\n",
    "        test_loader = TimeSeriesLoader(dataset=test_dataset,\n",
    "                                       batch_size=1,\n",
    "                                       shuffle=False)\n",
    "    else:\n",
    "        test_loader = None\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_nbeats(mc):\n",
    "    mc['n_theta_hidden'] = len(mc['stack_types']) * [ [int(mc['n_hidden']), int(mc['n_hidden'])] ]\n",
    "    model = NBEATS(n_time_in=int(mc['n_time_in']),\n",
    "                   n_time_out=int(mc['n_time_out']),\n",
    "                   n_x=mc['n_x'],\n",
    "                   n_s=mc['n_s'],\n",
    "                   n_s_hidden=int(mc['n_s_hidden']),\n",
    "                   n_x_hidden=int(mc['n_x_hidden']),\n",
    "                   shared_weights=mc['shared_weights'],\n",
    "                   initialization=mc['initialization'],\n",
    "                   activation=mc['activation'],\n",
    "                   stack_types=mc['stack_types'],\n",
    "                   n_blocks=mc['n_blocks'],\n",
    "                   n_layers=mc['n_layers'],\n",
    "                   n_theta_hidden=mc['n_theta_hidden'],\n",
    "                   n_harmonics=int(mc['n_harmonics']),\n",
    "                   n_polynomials=int(mc['n_polynomials']),\n",
    "                   batch_normalization = mc['batch_normalization'],\n",
    "                   dropout_prob_theta=mc['dropout_prob_theta'],\n",
    "                   learning_rate=float(mc['learning_rate']),\n",
    "                   lr_decay=float(mc['lr_decay']),\n",
    "                   lr_decay_step_size=float(mc['lr_decay_step_size']),\n",
    "                   weight_decay=mc['weight_decay'],\n",
    "                   loss_train=mc['loss_train'],\n",
    "                   loss_hypar=float(mc['loss_hypar']),\n",
    "                   loss_valid=mc['loss_valid'],\n",
    "                   frequency=mc['frequency'],\n",
    "                   seasonality=int(mc['seasonality']),\n",
    "                   random_seed=int(mc['random_seed']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_esrnn(mc):    \n",
    "    model = ESRNN(# Architecture parameters\n",
    "                  n_series=mc['n_series'],\n",
    "                  n_x=mc['n_x'],\n",
    "                  n_s=mc['n_s'],\n",
    "                  sample_freq=int(mc['sample_freq']),\n",
    "                  input_size=int(mc['n_time_in']),\n",
    "                  output_size=int(mc['n_time_out']),\n",
    "                  es_component=mc['es_component'],\n",
    "                  cell_type=mc['cell_type'],\n",
    "                  state_hsize=int(mc['state_hsize']),\n",
    "                  dilations=mc['dilations'],\n",
    "                  add_nl_layer=mc['add_nl_layer'],\n",
    "                  # Optimization parameters                \n",
    "                  learning_rate=mc['learning_rate'],\n",
    "                  lr_scheduler_step_size=int(mc['lr_decay_step_size']),\n",
    "                  lr_decay=mc['lr_decay'],\n",
    "                  per_series_lr_multip=mc['per_series_lr_multip'],\n",
    "                  gradient_eps=mc['gradient_eps'],\n",
    "                  gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                  rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                  noise_std=mc['noise_std'],\n",
    "                  level_variability_penalty=mc['level_variability_penalty'],\n",
    "                  testing_percentile=mc['testing_percentile'],\n",
    "                  training_percentile=mc['training_percentile'],\n",
    "                  loss=mc['loss_train'],\n",
    "                  val_loss=mc['loss_valid'],\n",
    "                  seasonality=mc['seasonality']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_mqesrnn(mc):    \n",
    "    model = MQESRNN(# Architecture parameters\n",
    "                    n_series=mc['n_series'],\n",
    "                    n_x=mc['n_x'],\n",
    "                    n_s=mc['n_s'],\n",
    "                    idx_to_sample_freq=int(mc['idx_to_sample_freq']),\n",
    "                    input_size=int(mc['n_time_in']),\n",
    "                    output_size=int(mc['n_time_out']),\n",
    "                    es_component=mc['es_component'],\n",
    "                    cell_type=mc['cell_type'],\n",
    "                    state_hsize=int(mc['state_hsize']),\n",
    "                    dilations=mc['dilations'],\n",
    "                    add_nl_layer=mc['add_nl_layer'],\n",
    "                    # Optimization parameters                 \n",
    "                    learning_rate=mc['learning_rate'],\n",
    "                    lr_scheduler_step_size=int(mc['lr_decay_step_size']),\n",
    "                    lr_decay=mc['lr_decay'],\n",
    "                    gradient_eps=mc['gradient_eps'],\n",
    "                    gradient_clipping_threshold=mc['gradient_clipping_threshold'],\n",
    "                    rnn_weight_decay=mc['rnn_weight_decay'],\n",
    "                    noise_std=mc['noise_std'],\n",
    "                    testing_percentiles=list(mc['testing_percentiles']),\n",
    "                    training_percentiles=list(mc['training_percentiles']),\n",
    "                    loss=mc['loss_train'],\n",
    "                    val_loss=mc['loss_valid']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def instantiate_model(mc):\n",
    "    MODEL_DICT = {'nbeats': instantiate_nbeats,\n",
    "                  'esrnn': instantiate_esrnn,\n",
    "                  'mqesrnn': instantiate_mqesrnn,}\n",
    "    return MODEL_DICT[mc['model']](mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_fit_predict(mc, S_df, Y_df, X_df, f_cols,\n",
    "                      ds_in_test, ds_in_val,\n",
    "                      n_uids, n_val_windows, freq,\n",
    "                      is_val_random):\n",
    "    \n",
    "    # Protect inplace modifications\n",
    "    Y_df = Y_df.copy()\n",
    "    if X_df is not None:\n",
    "        X_df = X_df.copy()\n",
    "    if S_df is not None:\n",
    "        S_df = S_df.copy()        \n",
    "\n",
    "    #----------------------------------------------- Datasets -----------------------------------------------#\n",
    "    train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=mc,\n",
    "                                                                         S_df=S_df, Y_df=Y_df, X_df=X_df,\n",
    "                                                                         f_cols=f_cols,\n",
    "                                                                         ds_in_test=ds_in_test,\n",
    "                                                                         ds_in_val=ds_in_val,\n",
    "                                                                         n_uids=n_uids, \n",
    "                                                                         n_val_windows=n_val_windows,\n",
    "                                                                         freq=freq, is_val_random=is_val_random)\n",
    "    mc['n_x'], mc['n_s'] = train_dataset.get_n_variables()\n",
    "\n",
    "    #------------------------------------------- Instantiate & fit -------------------------------------------#\n",
    "    train_loader, val_loader, test_loader = instantiate_loaders(mc=mc,\n",
    "                                                                train_dataset=train_dataset,\n",
    "                                                                val_dataset=val_dataset,\n",
    "                                                                test_dataset=test_dataset)\n",
    "    model = instantiate_model(mc=mc)\n",
    "    callbacks = []\n",
    "    if mc['early_stop_patience']:\n",
    "        early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, \n",
    "                                                    patience=mc['early_stop_patience'],\n",
    "                                                    verbose=True, \n",
    "                                                    mode='min') \n",
    "        callbacks=[early_stopping]\n",
    "\n",
    "    gpus = -1 if t.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(max_epochs=mc['max_epochs'], \n",
    "                         max_steps=mc['max_steps'],\n",
    "                         callbacks=callbacks)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    #------------------------------------------------ Predict ------------------------------------------------#\n",
    "    # Predict test if available\n",
    "    if ds_in_test > 0:\n",
    "        outputs = trainer.predict(model, test_loader)\n",
    "        y_true, y_hat, mask = [t.cat(output).cpu().numpy() for output in zip(*outputs)]\n",
    "        meta_data = test_loader.dataset.meta_data\n",
    "    else:\n",
    "        outputs = trainer.predict(model, val_loader)\n",
    "        y_true, y_hat, mask = [t.cat(output).cpu().numpy()[:, -1] for output in zip(*outputs)]\n",
    "        meta_data = val_loader.dataset.meta_data\n",
    "    \n",
    "    # Scale to original scale\n",
    "    if mc['normalizer_y'] is not None:\n",
    "        y_true_shape = y_true.shape\n",
    "        y_true = scaler_y.inv_scale(x=y_true.flatten())\n",
    "        y_true = np.reshape(y_true, y_true_shape)\n",
    "\n",
    "        y_hat = scaler_y.inv_scale(x=y_hat.flatten())\n",
    "        y_hat = np.reshape(y_hat, y_true_shape)\n",
    "\n",
    "    print(f\"y_true.shape (#n_series, #n_fcds, #lt): {y_true.shape}\")\n",
    "    print(f\"y_hat.shape (#n_series, #n_fcds, #lt): {y_hat.shape}\")\n",
    "    print(\"\\n\")\n",
    "    return y_true, y_hat, mask, meta_data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def evaluate_model(mc, loss_function, \n",
    "                   S_df, Y_df, X_df, f_cols,\n",
    "                   ds_in_test, ds_in_val,\n",
    "                   n_uids, n_val_windows, freq,\n",
    "                   is_val_random,\n",
    "                   loss_kwargs):\n",
    "    \n",
    "    print(47*'=' + '\\n')\n",
    "    print(pd.Series(mc))\n",
    "    print(47*'=' + '\\n')\n",
    "    \n",
    "    # Some asserts due to work in progress\n",
    "    n_series = Y_df['unique_id'].nunique()\n",
    "    if n_series > 1:\n",
    "        assert mc['normalizer_y'] is None, 'Data scaling not implemented with multiple time series'\n",
    "        assert mc['normalizer_x'] is None, 'Data scaling not implemented with multiple time series'\n",
    "\n",
    "    assert ds_in_test % mc['val_idx_to_sample_freq']==0, 'outsample size should be multiple of val_idx_to_sample_freq'\n",
    "\n",
    "    # Make predictions\n",
    "    start = time.time()\n",
    "    y_true, y_hat, mask, meta_data, model = model_fit_predict(mc=mc,\n",
    "                                                              S_df=S_df, \n",
    "                                                              Y_df=Y_df,\n",
    "                                                              X_df=X_df,\n",
    "                                                              f_cols=f_cols,\n",
    "                                                              ds_in_test=ds_in_test, \n",
    "                                                              ds_in_val=ds_in_val,\n",
    "                                                              n_uids=n_uids,\n",
    "                                                              n_val_windows=n_val_windows,\n",
    "                                                              freq=freq,\n",
    "                                                              is_val_random=is_val_random)\n",
    "    run_time = time.time() - start\n",
    "\n",
    "    # Evaluate predictions\n",
    "    loss = loss_function(y=y_true, y_hat=y_hat, weights=mask, **loss_kwargs)\n",
    "\n",
    "    result =  {'loss': loss,\n",
    "               'mc': mc,\n",
    "               'y_true': y_true,\n",
    "               'y_hat': y_hat,\n",
    "               'run_time': run_time,\n",
    "               'status': STATUS_OK}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hyperopt_tunning(space, hyperopt_max_evals, loss_function,\n",
    "                     S_df, Y_df, X_df, f_cols,\n",
    "                     ds_in_val,\n",
    "                     n_uids, n_val_windows, freq,\n",
    "                     is_val_random,\n",
    "                     save_trials=False,\n",
    "                     loss_kwargs=None):\n",
    "    trials = Trials()\n",
    "    fmin_objective = partial(evaluate_model, loss_function=loss_function, \n",
    "                             S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=f_cols,\n",
    "                             ds_in_test=0, ds_in_val=ds_in_val,\n",
    "                             n_uids=n_uids, n_val_windows=n_val_windows, freq=freq,\n",
    "                             is_val_random=is_val_random,\n",
    "                             loss_kwargs=loss_kwargs or {})\n",
    "\n",
    "    fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=hyperopt_max_evals, trials=trials, verbose=True)\n",
    "\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Utils Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nixtlats.losses.numpy import mae, mape, smape, rmse, pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if t.cuda.is_available(): device = 'cuda'  \n",
    "\n",
    "nbeats_space= {# Architecture parameters\n",
    "               'model':'nbeats',\n",
    "               'mode': 'simple',\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'n_x_hidden': hp.quniform('n_x_hidden', 1, 10, 1),\n",
    "               'n_s_hidden': hp.choice('n_s_hidden', [0]),\n",
    "               'shared_weights': hp.choice('shared_weights', [False]),\n",
    "               'activation': hp.choice('activation', ['SELU']),\n",
    "               'initialization':  hp.choice('initialization', ['glorot_normal','he_normal']),\n",
    "               'stack_types': hp.choice('stack_types', [2*['identity'],\n",
    "                                                        1*['identity']+1*['exogenous_tcn'],\n",
    "                                                        1*['exogenous_tcn']+1*['identity'] ]),\n",
    "               'n_blocks': hp.choice('n_blocks', [ [1, 1] ]),\n",
    "               'n_layers': hp.choice('n_layers', [ [2, 2] ]),\n",
    "               'n_hidden': hp.choice('n_hidden', [ 256 ]),\n",
    "               'n_harmonics': hp.choice('n_harmonics', [1]),\n",
    "               'n_polynomials': hp.choice('n_polynomials', [2]),\n",
    "               # Regularization and optimization parameters\n",
    "               'batch_normalization': hp.choice('batch_normalization', [False]),\n",
    "               'dropout_prob_theta': hp.uniform('dropout_prob_theta', 0, 0.5),\n",
    "               'dropout_prob_exogenous': hp.uniform('dropout_prob_exogenous', 0, 0.5),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.001)),\n",
    "               'lr_decay': hp.uniform('lr_decay', 0.3, 0.5),\n",
    "               'lr_decay_step_size': hp.choice('lr_decay_step_size', [100]), \n",
    "               'weight_decay': hp.loguniform('weight_decay', np.log(5e-5), np.log(5e-3)),\n",
    "               'max_epochs': hp.choice('max_epochs', [10]), #'n_iterations': hp.choice('n_iterations', [10])\n",
    "               'max_steps': hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience': hp.choice('early_stop_patience', [16]),\n",
    "               'eval_freq': hp.choice('eval_freq', [50]),\n",
    "               'n_val_weeks': hp.choice('n_val_weeks', [52*2]),\n",
    "               'loss_train': hp.choice('loss', ['MAE']),\n",
    "               'loss_hypar': hp.choice('loss_hypar', [0.5]),                \n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']), #[args.val_loss]),\n",
    "               'l1_theta': hp.choice('l1_theta', [0]),\n",
    "               # Data parameters\n",
    "               'len_sample_chunks': hp.choice('len_sample_chunks', [None]),\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x', ['median']),\n",
    "               'window_sampling_limit': hp.choice('window_sampling_limit', [100_000]),\n",
    "               'complete_inputs': hp.choice('complete_inputs', [False]),\n",
    "               'complete_sample': hp.choice('complete_sample', [False]),                \n",
    "               'frequency': hp.choice('frequency', ['H']),\n",
    "               'seasonality': hp.choice('seasonality', [24]),      \n",
    "               'idx_to_sample_freq': hp.choice('idx_to_sample_freq', [24]),\n",
    "               'val_idx_to_sample_freq': hp.choice('val_idx_to_sample_freq', [24]),\n",
    "               'batch_size': hp.choice('batch_size', [256]),\n",
    "               'n_series_per_batch': hp.choice('n_series_per_batch', [1]),\n",
    "               'random_seed': hp.quniform('random_seed', 10, 20, 1),\n",
    "               'device': hp.choice('device', [device])}\n",
    "\n",
    "mc = {'model':'nbeats',\n",
    "      # Architecture parameters\n",
    "      'n_time_in': 7*24,\n",
    "      'n_time_out': 24,\n",
    "      'n_x_hidden': 3,\n",
    "      'n_s_hidden': 0,\n",
    "      'shared_weights': False,\n",
    "      'activation': 'SELU',\n",
    "      'initialization': 'he_normal',\n",
    "      'stack_types': ['exogenous_tcn']+1*['identity'],\n",
    "      'n_blocks': [1, 1],\n",
    "      'n_layers': [2, 2],\n",
    "      'n_hidden': 364,\n",
    "      'n_polynomials': 2,\n",
    "      'n_harmonics': 1,\n",
    "      # Regularization and optimization parameters\n",
    "      'max_epochs': 10, #'n_iterations': 100,\n",
    "      'max_steps': None,      \n",
    "      'early_stop_patience': 8,\n",
    "      'batch_normalization': False,\n",
    "      'dropout_prob_theta': 0.2,\n",
    "      'learning_rate': 0.0005, #0.002,\n",
    "      'lr_decay': 0.64,\n",
    "      'lr_decay_step_size': 100,\n",
    "      'weight_decay': 0.00015,\n",
    "      'eval_freq': 50,\n",
    "      'n_val_weeks': 52*2,\n",
    "      'loss_train': 'PINBALL',\n",
    "      'loss_hypar': 0.5, #0.49,\n",
    "      'loss_valid': 'MAE',\n",
    "      'l1_theta': 0,\n",
    "      # Data parameters\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': 'median',\n",
    "      'window_sampling_limit': 100_000,\n",
    "      'complete_inputs': False,\n",
    "      'frequency':'H',\n",
    "      'seasonality': 24,\n",
    "      'idx_to_sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'batch_size': 256,\n",
    "      'n_series_per_batch': 1,\n",
    "      'random_seed': 10,\n",
    "      'device': 'cpu'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esrnn_space = {'model': hp.choice('model', ['esrnn']),\n",
    "               'mode': 'full',\n",
    "               # Architecture parameters\n",
    "               'n_time_in': hp.choice('n_time_in', [7*24]),\n",
    "               'n_time_out': hp.choice('n_time_out', [24]),\n",
    "               'dilations': hp.choice('dilations', [ [[1, 2]], [[1,2], [7, 14]] ]),\n",
    "               'es_component': hp.choice('es_component', ['multiplicative']),\n",
    "               'cell_type': hp.choice('cell_type', ['LSTM']),\n",
    "               'state_hsize': hp.quniform('state_hsize', 10, 100, 10),\n",
    "               'add_nl_layer': hp.choice('add_nl_layer', [True, False]),\n",
    "               'seasonality': hp.choice('seasonality', [ [24] ]),\n",
    "               # Regularization and optimization parameters\n",
    "               'max_epochs':hp.choice('max_epochs', [10]),\n",
    "               'max_steps':hp.choice('max_steps', [None]),\n",
    "               'early_stop_patience':hp.choice('early_stop_patience', [10]),\n",
    "               'eval_freq': hp.choice('eval_freq', [10]),\n",
    "               'batch_size': hp.choice('batch_size', [32]),\n",
    "               'learning_rate': hp.loguniform('learning_rate', np.log(5e-4), np.log(0.01)),\n",
    "               'lr_decay': hp.quniform('lr_decay', 0.5, 0.8, 0.1),\n",
    "               'lr_decay_step_size': hp.choice('lr_decay_step_size', [100]), \n",
    "               'per_series_lr_multip': hp.choice('per_series_lr_multip', [0.5, 1.0, 1.5, 2.0, 3.0]),\n",
    "               'gradient_eps': hp.choice('gradient_eps', [1e-8]),\n",
    "               'gradient_clipping_threshold': hp.choice('gradient_clipping_threshold', [10, 50]),\n",
    "               'rnn_weight_decay': hp.choice('rnn_weight_decay', [0, 0.0005, 0.005]),\n",
    "               'noise_std': hp.loguniform('noise_std', np.log(0.0001), np.log(0.001)),\n",
    "               'level_variability_penalty': hp.quniform('level_variability_penalty', 0, 100, 10),\n",
    "               'testing_percentile': hp.choice('testing_percentile', [50]),\n",
    "               'training_percentile': hp.choice('training_percentile', [48, 49, 50, 51]),\n",
    "               'random_seed': hp.quniform('random_seed', 1, 1000, 1),\n",
    "               'loss_train': hp.choice('loss_train', ['SMYL']),\n",
    "               'loss_valid': hp.choice('loss_valid', ['MAE']),\n",
    "               # Data parameters\n",
    "               'len_sample_chunks': hp.choice('len_sample_chunks', [7*3*24]),\n",
    "               'window_sampling_limit': hp.choice('window_sampling_limit', [500_000]),\n",
    "               'complete_inputs': hp.choice('complete_inputs', [True]),\n",
    "               'complete_sample': hp.choice('complete_sample', [True]),\n",
    "               'sample_freq': hp.choice('sample_freq', [24]),\n",
    "               'val_sample_freq': hp.choice('val_sample_freq', [24]),\n",
    "               'n_series_per_batch': hp.choice('n_series_per_batch', [1]),\n",
    "               'normalizer_y': hp.choice('normalizer_y', [None]),\n",
    "               'normalizer_x': hp.choice('normalizer_x',  [None])}\n",
    "\n",
    "mc = {'model':'esrnn',\n",
    "      'mode': 'full',\n",
    "      # Architecture parameters\n",
    "      'n_series': 1,\n",
    "      'n_time_in': 7*24,\n",
    "      'n_time_out': 24,\n",
    "      'n_x': 1,\n",
    "      'n_s': 1,\n",
    "      'dilations': [[1,2], [7]],\n",
    "      'es_component': 'multiplicative',\n",
    "      'cell_type': 'LSTM',\n",
    "      'state_hsize': 50,\n",
    "      'add_nl_layer': False,\n",
    "      'seasonality': [24],\n",
    "      # Regularization and optimization parameters\n",
    "      'max_epochs': 10, #'n_iterations': 100,\n",
    "      'max_steps': None,\n",
    "      'early_stop_patience': 10,\n",
    "      'eval_freq': 10,\n",
    "      'batch_size': 32,\n",
    "      'eq_batch_size': False,\n",
    "      'learning_rate': 0.0005,\n",
    "      'lr_decay': 0.8,\n",
    "      'lr_decay_step_size': 100,\n",
    "      'per_series_lr_multip': 1.5,\n",
    "      'gradient_eps': 1e-8, \n",
    "      'gradient_clipping_threshold': 20,\n",
    "      'rnn_weight_decay': 0.0,\n",
    "      'noise_std': 0.0005,\n",
    "      'level_variability_penalty': 10,\n",
    "      'testing_percentile': 50,\n",
    "      'training_percentile': 50,\n",
    "      'random_seed': 1,\n",
    "      'loss_train': 'SMYL',\n",
    "      'loss_valid': 'MAE',\n",
    "      # Data parameters\n",
    "      'len_sample_chunks': 7*4*24,\n",
    "      'window_sampling_limit': 500_000,\n",
    "      'complete_inputs': True,\n",
    "      'sample_freq': 24,\n",
    "      'val_idx_to_sample_freq': 24,\n",
    "      'n_series_per_batch': 1,\n",
    "      'normalizer_y': None,\n",
    "      'normalizer_x': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate_esrnn(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nixtlats.data.datasets.epf import EPF, EPFInfo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = ['NP']\n",
    "\n",
    "Y_df, X_df, S_df = EPF.load_groups(directory='data', groups=dataset)\n",
    "\n",
    "X_df = X_df[['unique_id', 'ds', 'week_day']]\n",
    "Y_min = Y_df.y.min()\n",
    "#Y_df.y = Y_df.y - Y_min + 20\n",
    "\n",
    "plt.plot(Y_df.y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: think about use of big windows during train for extremely long series\n",
    "# backpropagation trough time is slow\n",
    "# result = evaluate_model(loss_function=mae, mc=mc, \n",
    "#                         S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "#                         ds_in_test=0, ds_in_val=728*24,\n",
    "#                         n_uids=None, n_val_windows=None, freq=None,\n",
    "#                         is_val_random=False, loss_kwargs={})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(result['y_hat'].flatten())\n",
    "# plt.plot(Y_df['y'][-728*24:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.012577 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================      \n",
      "\n",
      "activation                                     SELU  \n",
      "batch_normalization                           False\n",
      "batch_size                                      256\n",
      "complete_inputs                               False\n",
      "complete_sample                               False\n",
      "device                                         cuda\n",
      "dropout_prob_exogenous                     0.246426\n",
      "dropout_prob_theta                         0.044532\n",
      "early_stop_patience                              16\n",
      "eval_freq                                        50\n",
      "frequency                                         H\n",
      "idx_to_sample_freq                               24\n",
      "initialization                            he_normal\n",
      "l1_theta                                          0\n",
      "learning_rate                               0.00059\n",
      "len_sample_chunks                              None\n",
      "loss_hypar                                      0.5\n",
      "loss_train                                      MAE\n",
      "loss_valid                                      MAE\n",
      "lr_decay                                   0.472903\n",
      "lr_decay_step_size                              100\n",
      "max_epochs                                       10\n",
      "max_steps                                      None\n",
      "mode                                         simple\n",
      "model                                        nbeats\n",
      "n_blocks                                     (1, 1)\n",
      "n_harmonics                                       1\n",
      "n_hidden                                        256\n",
      "n_layers                                     (2, 2)\n",
      "n_polynomials                                     2\n",
      "n_s_hidden                                        0\n",
      "n_series_per_batch                                1\n",
      "n_time_in                                       168\n",
      "n_time_out                                       24\n",
      "n_val_weeks                                     104\n",
      "n_x_hidden                                      5.0\n",
      "normalizer_x                                 median\n",
      "normalizer_y                                   None\n",
      "random_seed                                    16.0\n",
      "seasonality                                      24\n",
      "shared_weights                                False\n",
      "stack_types               (exogenous_tcn, identity)\n",
      "val_idx_to_sample_freq                           24\n",
      "weight_decay                               0.000058\n",
      "window_sampling_limit                        100000\n",
      "dtype: object\n",
      "===============================================      \n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2016-12-27 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=66.67, \t34944 time stamps \n",
      "Outsample percentage=33.33, \t17472 time stamps \n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2016-12-26 23:00:00\n",
      "          1           2016-12-27 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=33.33, \t17472 time stamps \n",
      "Outsample percentage=66.67, \t34944 time stamps \n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | _NBEATS | 368 K \n",
      "----------------------------------\n",
      "368 K     Trainable params\n",
      "0         Non-trainable params\n",
      "368 K     Total params\n",
      "1.475     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df03e32b6f864b14ae81c35e574b1878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7e06a22b694eefb47f8c69a68b2717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5288dd665b34f2a88147209e3d39866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 12.557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec69c04742714700a2b1ae2d26e20eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 6.057 >= min_delta = 0.0001. New best score: 6.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ffbd0afbec4e3d8e42e82f0d00e65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.609 >= min_delta = 0.0001. New best score: 4.891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2937222421a84d8991352d1faf66419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.366 >= min_delta = 0.0001. New best score: 4.525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f07a0886d264fa6840aba7f638420e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b207c80d0ea54c259226d74c6645ea92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.341 >= min_delta = 0.0001. New best score: 3.184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa627b6ea044e58a8ad1a55d374c86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f917a6e6e94181a353cc9771171413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d277d5694444a0a8875b95087c5f23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.115 >= min_delta = 0.0001. New best score: 3.069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df20107720840a3bea4597dc4a7bdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6cd85eeb074efc856ada6ca9dae6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true.shape (#n_series, #n_fcds, #lt): (728,)       \n",
      "y_hat.shape (#n_series, #n_fcds, #lt): (728,)        \n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.67s/trial, best loss: 3.701502561569214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.013880 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 3.701503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================                               \n",
      "\n",
      "activation                                SELU                                \n",
      "batch_normalization                      False\n",
      "batch_size                                 256\n",
      "complete_inputs                          False\n",
      "complete_sample                          False\n",
      "device                                    cuda\n",
      "dropout_prob_exogenous                0.365531\n",
      "dropout_prob_theta                    0.241073\n",
      "early_stop_patience                         16\n",
      "eval_freq                                   50\n",
      "frequency                                    H\n",
      "idx_to_sample_freq                          24\n",
      "initialization                       he_normal\n",
      "l1_theta                                     0\n",
      "learning_rate                         0.000853\n",
      "len_sample_chunks                         None\n",
      "loss_hypar                                 0.5\n",
      "loss_train                                 MAE\n",
      "loss_valid                                 MAE\n",
      "lr_decay                              0.355323\n",
      "lr_decay_step_size                         100\n",
      "max_epochs                                  10\n",
      "max_steps                                 None\n",
      "mode                                    simple\n",
      "model                                   nbeats\n",
      "n_blocks                                (1, 1)\n",
      "n_harmonics                                  1\n",
      "n_hidden                                   256\n",
      "n_layers                                (2, 2)\n",
      "n_polynomials                                2\n",
      "n_s_hidden                                   0\n",
      "n_series_per_batch                           1\n",
      "n_time_in                                  168\n",
      "n_time_out                                  24\n",
      "n_val_weeks                                104\n",
      "n_x_hidden                                 6.0\n",
      "normalizer_x                            median\n",
      "normalizer_y                              None\n",
      "random_seed                               18.0\n",
      "seasonality                                 24\n",
      "shared_weights                           False\n",
      "stack_types               (identity, identity)\n",
      "val_idx_to_sample_freq                      24\n",
      "weight_decay                          0.000123\n",
      "window_sampling_limit                   100000\n",
      "dtype: object\n",
      "===============================================                               \n",
      "\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.67s/trial, best loss: 3.701502561569214]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2016-12-27 2018-12-24 23:00:00\n",
      "          1           2013-01-01 2016-12-26 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=66.67, \t34944 time stamps \n",
      "Outsample percentage=33.33, \t17472 time stamps \n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2016-12-26 23:00:00\n",
      "          1           2016-12-27 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=33.33, \t17472 time stamps \n",
      "Outsample percentage=66.67, \t34944 time stamps \n",
      "\n",
      "INFO:root:Train Validation splits\n",
      "\n",
      "INFO:root:                              ds                    \n",
      "                             min                 max\n",
      "unique_id sample_mask                               \n",
      "NP        0           2013-01-01 2018-12-24 23:00:00\n",
      "INFO:root:\n",
      "Total data \t\t\t52416 time stamps \n",
      "Available percentage=100.0, \t52416 time stamps \n",
      "Insample  percentage=0.0, \t0 time stamps \n",
      "Outsample percentage=100.0, \t52416 time stamps \n",
      "\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | _NBEATS | 415 K \n",
      "----------------------------------\n",
      "415 K     Trainable params\n",
      "0         Non-trainable params\n",
      "415 K     Total params\n",
      "1.660     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432965401734439bb5e76073a7e2ca06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ec1324f34b4fb88a8a939988051be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd3ede6cd8d4678b82546da2d9e8b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 4.409\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421ee445943e451dbaac07c15546077d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.711 >= min_delta = 0.0001. New best score: 3.698\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de44aeeeaff4344a09df448e82613a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.311 >= min_delta = 0.0001. New best score: 3.388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44a8fb8189a41649e521013a718e7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.352 >= min_delta = 0.0001. New best score: 3.035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50222d3697c64a3ea3ee57a028046d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3738082e894acdaf181a44c0b6bf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0001. New best score: 3.032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36af398f792c4d698cd75488e18ab076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.147 >= min_delta = 0.0001. New best score: 2.885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fee472a39941bba92d3cf542626019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.074 >= min_delta = 0.0001. New best score: 2.811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b220af9e604f4f8b9330ad6b66e64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07cf347a2734fb4aefb7472e060593d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236c42419a594e6a9908b5879d91e1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true.shape (#n_series, #n_fcds, #lt): (728,)                                \n",
      "y_hat.shape (#n_series, #n_fcds, #lt): (728,)                                 \n",
      "100%|██████████| 2/2 [00:04<00:00,  2.42s/trial, best loss: 1.9704331159591675]\n"
     ]
    }
   ],
   "source": [
    "trials = hyperopt_tunning(space=nbeats_space, hyperopt_max_evals=2, loss_function=mae,\n",
    "                          S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],\n",
    "                          ds_in_val=728*24, n_uids=None, n_val_windows=None, freq=None,\n",
    "                          is_val_random=False, loss_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 2,\n",
       "  'tid': 0,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 3.701502561569214,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_inputs': False,\n",
       "    'complete_sample': False,\n",
       "    'device': 'cuda',\n",
       "    'dropout_prob_exogenous': 0.24642550819005665,\n",
       "    'dropout_prob_theta': 0.04453225609606154,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0005903125740803596,\n",
       "    'len_sample_chunks': None,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.47290326767085267,\n",
       "    'lr_decay_step_size': 100,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'nbeats',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_harmonics': 1,\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_polynomials': 2,\n",
       "    'n_s_hidden': 0,\n",
       "    'n_series_per_batch': 1,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_val_weeks': 104,\n",
       "    'n_x_hidden': 5.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 16.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('exogenous_tcn', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 5.8073579520205266e-05,\n",
       "    'window_sampling_limit': 100000,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'y_true': array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,\n",
       "          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,\n",
       "          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,\n",
       "          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,\n",
       "          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,\n",
       "          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,\n",
       "          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,\n",
       "          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,\n",
       "          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,\n",
       "          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,\n",
       "          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,\n",
       "          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,\n",
       "          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,\n",
       "          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,\n",
       "          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,\n",
       "          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,\n",
       "          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,\n",
       "          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,\n",
       "          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,\n",
       "          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,\n",
       "          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,\n",
       "          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,\n",
       "          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,\n",
       "          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,\n",
       "          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,\n",
       "          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,\n",
       "          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,\n",
       "          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,\n",
       "          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,\n",
       "          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,\n",
       "          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,\n",
       "          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,\n",
       "          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,\n",
       "          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,\n",
       "          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,\n",
       "          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,\n",
       "          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,\n",
       "          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,\n",
       "          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,\n",
       "          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,\n",
       "          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,\n",
       "          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,\n",
       "          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,\n",
       "          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,\n",
       "          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,\n",
       "          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,\n",
       "          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,\n",
       "          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,\n",
       "          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,\n",
       "          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,\n",
       "          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,\n",
       "          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,\n",
       "          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,\n",
       "          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,\n",
       "          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,\n",
       "          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,\n",
       "          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,\n",
       "          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,\n",
       "          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,\n",
       "          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,\n",
       "          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,\n",
       "          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,\n",
       "          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,\n",
       "          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,\n",
       "          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,\n",
       "          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,\n",
       "          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,\n",
       "          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,\n",
       "          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,\n",
       "          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,\n",
       "          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,\n",
       "          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,\n",
       "          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,\n",
       "          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,\n",
       "          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,\n",
       "          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,\n",
       "          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,\n",
       "          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,\n",
       "          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,\n",
       "          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,\n",
       "          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],\n",
       "         dtype=float32),\n",
       "   'y_hat': array([27.701649 , 27.975748 , 31.803595 , 31.472857 , 28.643927 ,\n",
       "          28.219172 , 31.853521 , 33.827473 , 30.845858 , 33.125107 ,\n",
       "          36.181877 , 31.319834 , 33.202126 , 32.05909  , 31.78731  ,\n",
       "          31.477406 , 27.625643 , 31.886206 , 33.18131  , 31.663424 ,\n",
       "          33.47664  , 33.102306 , 30.76027  , 30.854624 , 30.513063 ,\n",
       "          31.119108 , 31.45578  , 30.727148 , 31.015617 , 31.072279 ,\n",
       "          29.91333  , 31.884443 , 31.577305 , 31.11768  , 30.809654 ,\n",
       "          31.933662 , 32.662987 , 31.777435 , 33.141994 , 33.583065 ,\n",
       "          33.415916 , 33.67277  , 33.67393  , 34.7538   , 35.73098  ,\n",
       "          34.790936 , 33.86699  , 33.681534 , 33.691025 , 35.487526 ,\n",
       "          35.726833 , 34.177856 , 33.04427  , 33.61656  , 30.676077 ,\n",
       "          32.29714  , 30.447182 , 32.80938  , 29.365845 , 30.978231 ,\n",
       "          33.46982  , 31.01993  , 31.37819  , 30.004261 , 32.07784  ,\n",
       "          33.05431  , 32.186184 , 34.075848 , 31.757715 , 33.468113 ,\n",
       "          33.654778 , 34.731728 , 32.142467 , 33.155827 , 34.23377  ,\n",
       "          32.650238 , 32.461346 , 32.81042  , 27.75075  , 31.551151 ,\n",
       "          30.965195 , 28.564816 , 31.68756  , 29.758919 , 29.686258 ,\n",
       "          30.2022   , 31.718147 , 32.293495 , 31.605698 , 30.203241 ,\n",
       "          30.906057 , 31.021599 , 31.610073 , 32.170662 , 30.912388 ,\n",
       "          30.811941 , 29.598139 , 30.292961 , 30.65223  , 29.147573 ,\n",
       "          27.314524 , 29.550224 , 29.97744  , 29.968029 , 28.101295 ,\n",
       "          25.158245 , 27.554085 , 27.501097 , 30.007065 , 31.786043 ,\n",
       "          29.360703 , 31.8217   , 31.33803  , 32.28236  , 32.072117 ,\n",
       "          29.792221 , 27.287334 , 28.04121  , 32.11424  , 31.66252  ,\n",
       "          33.691414 , 33.059284 , 33.28169  , 32.11442  , 31.789    ,\n",
       "          30.190199 , 29.862118 , 31.577236 , 30.565132 , 30.82904  ,\n",
       "          32.587406 , 32.69837  , 30.077812 , 33.508186 , 34.395542 ,\n",
       "          34.74857  , 31.200544 , 32.430794 , 32.262714 , 33.08537  ,\n",
       "          33.395508 , 29.128036 , 29.362097 , 29.608278 , 27.92922  ,\n",
       "          27.02654  , 26.394283 , 28.339012 , 27.25325  , 27.914608 ,\n",
       "          27.493322 , 28.818203 , 26.07845  , 26.297255 , 28.600729 ,\n",
       "          27.653435 , 27.809496 , 28.893476 , 28.784111 , 28.29351  ,\n",
       "          28.631182 , 28.20375  , 15.827183 , 20.83848  , 28.532713 ,\n",
       "          26.407932 , 26.380068 , 25.573887 , 21.184254 , 26.372822 ,\n",
       "          26.85384  , 27.186338 , 24.613485 , 24.034958 , 26.529345 ,\n",
       "          27.095375 , 26.01566  , 28.533981 , 28.779293 , 26.78483  ,\n",
       "          25.258823 , 24.121305 , 25.542557 , 26.027983 , 25.805328 ,\n",
       "          26.068983 , 25.050343 , 25.963276 , 26.386597 , 26.41921  ,\n",
       "          27.66286  , 28.730894 , 29.326147 , 29.848225 , 28.787792 ,\n",
       "          29.89353  , 29.584927 , 30.182766 , 29.392458 , 29.99966  ,\n",
       "          29.57572  , 28.808083 , 28.217983 , 27.245605 , 27.927732 ,\n",
       "          28.180653 , 29.684679 , 30.347237 , 28.715103 , 29.052837 ,\n",
       "          29.060444 , 29.475107 , 29.56176  , 30.340288 , 29.287489 ,\n",
       "          29.029518 , 28.780752 , 28.22973  , 29.160313 , 29.684525 ,\n",
       "          27.260353 , 26.054703 , 26.877941 , 28.052938 , 28.59358  ,\n",
       "          26.314325 , 28.004002 , 28.309336 , 27.970922 , 24.17208  ,\n",
       "          28.47196  , 28.379786 , 28.33188  , 29.347889 , 28.264673 ,\n",
       "          28.490177 , 26.092579 , 28.270226 , 29.259161 , 30.01529  ,\n",
       "          30.176893 , 30.951273 , 31.655579 , 31.866587 , 33.29403  ,\n",
       "          32.32773  , 32.391518 , 33.013805 , 34.2049   , 35.39885  ,\n",
       "          33.71306  , 34.747    , 34.328926 , 34.239143 , 32.38846  ,\n",
       "          33.92605  , 32.34575  , 32.566185 , 30.322432 , 30.019318 ,\n",
       "          31.097141 , 29.253351 , 31.422064 , 33.120487 , 33.34096  ,\n",
       "          34.198757 , 33.978367 , 34.00793  , 34.345467 , 33.55096  ,\n",
       "          33.821102 , 33.069893 , 32.86123  , 32.431793 , 32.791203 ,\n",
       "          32.479095 , 33.151684 , 31.260265 , 30.307457 , 26.91224  ,\n",
       "          22.465515 , 26.214489 , 18.628647 , 28.102814 , 28.87575  ,\n",
       "          27.881306 , 30.460846 , 31.438694 , 30.465012 , 29.497032 ,\n",
       "          30.0089   , 29.787655 , 28.00818  , 24.77789  , 27.852478 ,\n",
       "          27.819096 , 30.92728  , 30.650131 , 30.579054 , 28.079836 ,\n",
       "          31.493883 , 32.205677 , 29.632847 , 29.815783 , 30.475258 ,\n",
       "          23.096104 , 22.133955 , 22.233164 , 30.209286 , 31.261034 ,\n",
       "          26.111069 , 31.214556 , 29.260584 , 27.040693 , 30.788288 ,\n",
       "          30.552942 , 30.781445 , 31.151209 , 28.85097  , 28.95206  ,\n",
       "          30.680902 , 32.994823 , 33.09333  , 32.04963  , 32.48223  ,\n",
       "          30.602373 , 30.763361 , 30.8481   , 31.841734 , 34.812283 ,\n",
       "          30.905994 , 28.58185  , 28.962284 , 31.419739 , 31.667522 ,\n",
       "          30.726204 , 30.979689 , 32.892284 , 35.24731  , 32.386612 ,\n",
       "          32.536953 , 27.545795 , 30.979244 , 34.07229  , 29.211065 ,\n",
       "          31.310766 , 27.148113 , 29.085176 , 29.240799 , 30.554121 ,\n",
       "          30.748781 , 28.725815 , 28.75438  , 30.609417 , 32.094326 ,\n",
       "          30.310951 , 31.347704 , 32.299927 , 31.04108  , 28.933968 ,\n",
       "          29.49194  , 29.331661 , 22.787268 , 28.163752 , 27.508389 ,\n",
       "          27.15601  , 30.905706 , 29.17673  , 28.989784 , 27.551468 ,\n",
       "          27.799349 , 27.869843 , 30.540209 , 29.43601  , 31.478884 ,\n",
       "          31.646564 , 31.34626  , 29.995522 , 30.587994 , 30.872017 ,\n",
       "          33.53783  , 31.656082 , 31.184862 , 29.8596   , 31.200302 ,\n",
       "          28.988365 , 31.1772   , 32.04352  , 33.33186  , 34.686695 ,\n",
       "          32.270588 , 32.81985  , 34.82245  , 30.255907 , 27.000778 ,\n",
       "          31.60639  , 33.21228  , 30.502766 , 29.421745 , 30.67129  ,\n",
       "          33.128506 , 31.022188 , 32.806087 , 34.58911  , 34.167183 ,\n",
       "          35.84772  , 36.8175   , 34.41109  , 34.334137 , 32.636158 ,\n",
       "          33.382095 , 32.60263  , 32.761208 , 34.902267 , 33.970512 ,\n",
       "          32.612194 , 35.29174  , 35.63135  , 37.379665 , 37.15835  ,\n",
       "          37.996864 , 38.55955  , 39.62931  , 40.94131  , 39.045547 ,\n",
       "          39.259083 , 40.67536  , 43.198765 , 41.556595 , 40.81526  ,\n",
       "          40.75979  , 39.169403 , 40.062454 , 39.544426 , 43.903584 ,\n",
       "          42.37286  , 42.325348 , 38.61     , 42.271305 , 40.341484 ,\n",
       "          40.199306 , 40.85454  , 40.683975 , 41.333935 , 40.204926 ,\n",
       "          41.462906 , 39.571663 , 41.945198 , 41.608612 , 42.790592 ,\n",
       "          41.463665 , 45.25569  , 43.36491  , 41.163616 , 43.871017 ,\n",
       "          45.4927   , 42.59713  , 41.977512 , 45.38557  , 44.078102 ,\n",
       "          43.563152 , 43.628407 , 43.38679  , 43.35113  , 42.114872 ,\n",
       "          41.94418  , 41.04163  , 42.020454 , 43.02959  , 41.9142   ,\n",
       "          40.711254 , 42.9427   , 42.758297 , 42.442474 , 43.66887  ,\n",
       "          43.3195   , 45.293713 , 42.29823  , 41.17005  , 38.19075  ,\n",
       "          36.101604 , 38.252167 , 38.092922 , 37.095108 , 37.838417 ,\n",
       "          37.761803 , 39.38203  , 39.604942 , 38.35947  , 37.192986 ,\n",
       "          34.91715  , 34.0571   , 39.86787  , 38.606857 , 37.521458 ,\n",
       "          29.980778 , 29.033047 , 27.31145  , 13.800947 ,  6.0904064,\n",
       "           9.970101 , 30.535395 , 20.79597  , 20.864275 , 32.450157 ,\n",
       "          33.952015 , 26.99879  , 30.519081 , 34.91247  , 33.947    ,\n",
       "          17.917217 , 28.553782 , 37.87419  , 39.01217  , 39.743526 ,\n",
       "          40.577442 , 40.99245  , 41.410915 , 42.328983 , 41.977943 ,\n",
       "          46.016853 , 45.31137  , 48.043182 , 45.53443  , 44.655006 ,\n",
       "          46.543743 , 48.812256 , 49.53628  , 50.94025  , 51.33296  ,\n",
       "          48.679546 , 51.131973 , 49.402878 , 50.521526 , 51.516975 ,\n",
       "          45.923653 , 50.18677  , 48.530865 , 49.261944 , 45.76501  ,\n",
       "          46.07821  , 46.40535  , 42.76134  , 42.00964  , 44.957035 ,\n",
       "          47.39794  , 47.693336 , 48.011383 , 49.165527 , 46.89182  ,\n",
       "          49.04584  , 49.60355  , 51.375633 , 53.59237  , 54.29658  ,\n",
       "          55.176777 , 54.874542 , 52.320477 , 52.85983  , 55.06989  ,\n",
       "          54.72569  , 55.65646  , 56.23277  , 56.42413  , 57.19465  ,\n",
       "          57.115982 , 58.316833 , 56.682034 , 56.355537 , 55.877407 ,\n",
       "          57.45604  , 58.18929  , 57.33557  , 58.4829   , 58.71005  ,\n",
       "          59.28448  , 59.773758 , 57.70367  , 56.553947 , 55.581455 ,\n",
       "          58.48933  , 57.707726 , 57.668633 , 58.90658  , 58.638313 ,\n",
       "          58.92934  , 54.6363   , 56.973274 , 58.90937  , 57.008236 ,\n",
       "          55.968906 , 52.896873 , 54.284573 , 54.306793 , 54.622375 ,\n",
       "          54.832138 , 55.11455  , 55.21072  , 53.679844 , 53.95129  ,\n",
       "          48.443203 , 53.31453  , 53.152607 , 54.75939  , 53.162964 ,\n",
       "          55.702606 , 54.479435 , 54.600582 , 55.40517  , 55.008972 ,\n",
       "          61.249275 , 56.707996 , 58.786995 , 61.139122 , 59.757816 ,\n",
       "          59.712875 , 59.467102 , 61.925026 , 62.40662  , 60.2509   ,\n",
       "          58.961067 , 58.977623 , 59.357323 , 60.032764 , 56.529358 ,\n",
       "          55.562675 , 59.210716 , 56.242374 , 54.238422 , 53.827488 ,\n",
       "          53.938107 , 49.232834 , 43.94818  , 47.780464 ,  7.518135 ,\n",
       "          22.608507 , 34.081154 , 40.401752 , 37.462734 , 21.43678  ,\n",
       "          31.341328 , 43.646458 , 22.532    , 44.00136  , 44.447636 ,\n",
       "          44.912804 , 48.062458 , 46.74743  , 49.90155  , 49.416477 ,\n",
       "          51.37282  , 48.461166 , 45.72746  , 46.469196 , 46.934605 ,\n",
       "          44.053886 , 22.644484 ,  6.8766646, 42.375244 , 44.543037 ,\n",
       "          44.539543 , 42.44923  , 43.2529   , 43.820484 , 39.382084 ,\n",
       "          37.480038 , 32.197952 , 44.780067 , 48.77445  , 46.61584  ,\n",
       "          46.138626 , 43.456142 , 42.042835 , 43.18084  , 44.793903 ,\n",
       "          46.872726 , 47.292267 , 45.51595  , 46.30709  , 47.181713 ,\n",
       "          48.00544  , 47.559364 , 48.243324 , 46.930027 , 43.474533 ,\n",
       "          43.177113 , 46.444042 , 48.409523 , 49.503876 , 48.877235 ,\n",
       "          48.031235 , 49.474457 , 49.27266  , 47.87698  , 48.969242 ,\n",
       "          52.262043 , 54.22861  , 51.19327  , 52.31781  , 52.05568  ,\n",
       "          54.560577 , 54.59873  , 45.36566  , 47.72178  , 47.66583  ,\n",
       "          49.503857 , 48.44381  , 48.557514 , 49.059517 , 51.484367 ,\n",
       "          51.29379  , 47.785267 , 47.74033  , 48.47073  , 49.527966 ,\n",
       "          53.69879  , 53.383175 , 52.97548  , 55.211697 , 48.94556  ,\n",
       "          54.80213  , 57.946636 , 52.23061  , 55.074226 , 52.95395  ,\n",
       "          54.286552 , 56.05054  , 57.66334  ], dtype=float32),\n",
       "   'run_time': 2.62833571434021,\n",
       "   'status': 'ok'},\n",
       "  'misc': {'tid': 0,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0],\n",
       "    'dropout_prob_theta': [0],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [0],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_harmonics': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_polynomials': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'n_x_hidden': [0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0],\n",
       "    'window_sampling_limit': [0]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.24642550819005665],\n",
       "    'dropout_prob_theta': [0.04453225609606154],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0005903125740803596],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.47290326767085267],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_harmonics': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_polynomials': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'n_x_hidden': [5.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [16.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [2],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [5.8073579520205266e-05],\n",
       "    'window_sampling_limit': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 6, 16, 16, 19, 26, 482000),\n",
       "  'refresh_time': datetime.datetime(2021, 6, 16, 16, 19, 29, 129000)},\n",
       " {'state': 2,\n",
       "  'tid': 1,\n",
       "  'spec': None,\n",
       "  'result': {'loss': 1.9704331159591675,\n",
       "   'mc': {'activation': 'SELU',\n",
       "    'batch_normalization': False,\n",
       "    'batch_size': 256,\n",
       "    'complete_inputs': False,\n",
       "    'complete_sample': False,\n",
       "    'device': 'cuda',\n",
       "    'dropout_prob_exogenous': 0.36553112690476813,\n",
       "    'dropout_prob_theta': 0.24107333575459605,\n",
       "    'early_stop_patience': 16,\n",
       "    'eval_freq': 50,\n",
       "    'frequency': 'H',\n",
       "    'idx_to_sample_freq': 24,\n",
       "    'initialization': 'he_normal',\n",
       "    'l1_theta': 0,\n",
       "    'learning_rate': 0.0008532112183090035,\n",
       "    'len_sample_chunks': None,\n",
       "    'loss_hypar': 0.5,\n",
       "    'loss_train': 'MAE',\n",
       "    'loss_valid': 'MAE',\n",
       "    'lr_decay': 0.3553227609462469,\n",
       "    'lr_decay_step_size': 100,\n",
       "    'max_epochs': 10,\n",
       "    'max_steps': None,\n",
       "    'mode': 'simple',\n",
       "    'model': 'nbeats',\n",
       "    'n_blocks': (1, 1),\n",
       "    'n_harmonics': 1,\n",
       "    'n_hidden': 256,\n",
       "    'n_layers': (2, 2),\n",
       "    'n_polynomials': 2,\n",
       "    'n_s_hidden': 0,\n",
       "    'n_series_per_batch': 1,\n",
       "    'n_time_in': 168,\n",
       "    'n_time_out': 24,\n",
       "    'n_val_weeks': 104,\n",
       "    'n_x_hidden': 6.0,\n",
       "    'normalizer_x': 'median',\n",
       "    'normalizer_y': None,\n",
       "    'random_seed': 18.0,\n",
       "    'seasonality': 24,\n",
       "    'shared_weights': False,\n",
       "    'stack_types': ('identity', 'identity'),\n",
       "    'val_idx_to_sample_freq': 24,\n",
       "    'weight_decay': 0.00012349475141123045,\n",
       "    'window_sampling_limit': 100000,\n",
       "    'n_x': 1,\n",
       "    'n_s': 1,\n",
       "    'n_theta_hidden': [[256, 256], [256, 256]]},\n",
       "   'y_true': array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,\n",
       "          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,\n",
       "          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,\n",
       "          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,\n",
       "          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,\n",
       "          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,\n",
       "          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,\n",
       "          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,\n",
       "          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,\n",
       "          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,\n",
       "          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,\n",
       "          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,\n",
       "          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,\n",
       "          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,\n",
       "          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,\n",
       "          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,\n",
       "          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,\n",
       "          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,\n",
       "          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,\n",
       "          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,\n",
       "          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,\n",
       "          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,\n",
       "          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,\n",
       "          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,\n",
       "          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,\n",
       "          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,\n",
       "          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,\n",
       "          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,\n",
       "          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,\n",
       "          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,\n",
       "          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,\n",
       "          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,\n",
       "          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,\n",
       "          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,\n",
       "          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,\n",
       "          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,\n",
       "          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,\n",
       "          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,\n",
       "          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,\n",
       "          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,\n",
       "          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,\n",
       "          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,\n",
       "          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,\n",
       "          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,\n",
       "          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,\n",
       "          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,\n",
       "          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,\n",
       "          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,\n",
       "          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,\n",
       "          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,\n",
       "          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,\n",
       "          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,\n",
       "          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,\n",
       "          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,\n",
       "          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,\n",
       "          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,\n",
       "          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,\n",
       "          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,\n",
       "          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,\n",
       "          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,\n",
       "          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,\n",
       "          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,\n",
       "          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,\n",
       "          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,\n",
       "          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,\n",
       "          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,\n",
       "          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,\n",
       "          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,\n",
       "          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,\n",
       "          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,\n",
       "          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,\n",
       "          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,\n",
       "          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,\n",
       "          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,\n",
       "          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,\n",
       "          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,\n",
       "          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,\n",
       "          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,\n",
       "          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,\n",
       "          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,\n",
       "          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],\n",
       "         dtype=float32),\n",
       "   'y_hat': array([26.316542 , 26.332678 , 29.868633 , 29.212381 , 26.364874 ,\n",
       "          26.917328 , 29.745722 , 31.536858 , 28.970425 , 31.199947 ,\n",
       "          33.604595 , 29.44432  , 31.221601 , 30.21905  , 29.22446  ,\n",
       "          28.915838 , 25.89044  , 29.230467 , 30.18915  , 29.975319 ,\n",
       "          31.366655 , 31.25101  , 30.071215 , 28.731358 , 28.535212 ,\n",
       "          27.618187 , 29.029678 , 29.363562 , 29.049566 , 28.913328 ,\n",
       "          28.011992 , 29.117641 , 29.140635 , 29.044228 , 28.66233  ,\n",
       "          29.913248 , 30.595543 , 29.616873 , 30.44652  , 31.032312 ,\n",
       "          31.402649 , 31.28904  , 31.644335 , 32.50862  , 33.501453 ,\n",
       "          32.649067 , 32.44691  , 32.40678  , 31.825151 , 32.10069  ,\n",
       "          33.180458 , 31.670181 , 30.474667 , 30.778875 , 28.836325 ,\n",
       "          29.560535 , 28.58201  , 30.497644 , 26.998016 , 28.269503 ,\n",
       "          30.825726 , 28.996218 , 29.206642 , 28.11583  , 30.245121 ,\n",
       "          30.50653  , 29.545559 , 31.27154  , 29.62038  , 31.091951 ,\n",
       "          31.401278 , 32.87783  , 30.627516 , 30.204292 , 32.011948 ,\n",
       "          30.288706 , 30.168816 , 30.597305 , 25.951916 , 29.145744 ,\n",
       "          28.223104 , 26.144302 , 29.699013 , 27.67291  , 27.693502 ,\n",
       "          28.665813 , 29.434685 , 29.595596 , 28.981207 , 28.706457 ,\n",
       "          28.617367 , 29.054865 , 29.54745  , 30.282429 , 28.1299   ,\n",
       "          28.439993 , 27.934156 , 28.018017 , 28.524834 , 27.756063 ,\n",
       "          25.052336 , 26.91068  , 27.404531 , 28.184435 , 26.043272 ,\n",
       "          23.491674 , 25.962397 , 25.21885  , 27.582739 , 29.087084 ,\n",
       "          27.533033 , 29.615124 , 29.390865 , 30.074804 , 30.195353 ,\n",
       "          27.71758  , 24.908722 , 26.24871  , 29.815231 , 29.513865 ,\n",
       "          31.788504 , 31.33913  , 31.55614  , 30.190062 , 30.616823 ,\n",
       "          27.675676 , 27.844856 , 28.76709  , 28.721764 , 27.732212 ,\n",
       "          30.144617 , 30.558546 , 27.959848 , 31.511566 , 32.05157  ,\n",
       "          32.824352 , 28.970306 , 30.320168 , 30.302729 , 31.071394 ,\n",
       "          30.95294  , 27.713512 , 27.281979 , 27.102125 , 25.239094 ,\n",
       "          25.68945  , 24.52259  , 26.123549 , 26.075962 , 26.127329 ,\n",
       "          25.531578 , 26.201128 , 24.493105 , 24.480654 , 26.935532 ,\n",
       "          26.475863 , 25.935757 , 26.59186  , 26.857464 , 26.650738 ,\n",
       "          26.893343 , 26.680841 , 14.411107 , 19.875776 , 26.351929 ,\n",
       "          24.763182 , 24.917603 , 24.733768 , 20.66013  , 25.580776 ,\n",
       "          24.815365 , 25.177776 , 22.208311 , 23.26691  , 24.852436 ,\n",
       "          25.302544 , 24.503868 , 26.88481  , 26.467033 , 24.698137 ,\n",
       "          23.865673 , 22.090553 , 23.847254 , 24.60143  , 24.05421  ,\n",
       "          23.585033 , 23.06441  , 24.394993 , 24.594437 , 24.765886 ,\n",
       "          26.094141 , 26.627785 , 26.85206  , 27.626198 , 27.08107  ,\n",
       "          27.824722 , 27.614399 , 27.995546 , 27.070292 , 27.359705 ,\n",
       "          26.868534 , 26.771917 , 26.021866 , 25.52235  , 26.10642  ,\n",
       "          25.980162 , 27.049232 , 27.784496 , 26.839733 , 27.043272 ,\n",
       "          27.195377 , 27.59984  , 27.399073 , 27.767735 , 26.6787   ,\n",
       "          27.154633 , 26.630852 , 26.905252 , 27.053562 , 27.463858 ,\n",
       "          25.143562 , 23.751587 , 25.07697  , 25.762617 , 26.608791 ,\n",
       "          25.115149 , 26.088032 , 25.91419  , 25.679714 , 22.293678 ,\n",
       "          26.299696 , 26.720356 , 26.651005 , 27.149632 , 25.823586 ,\n",
       "          26.134111 , 24.134195 , 26.175089 , 27.44743  , 28.230444 ,\n",
       "          28.000671 , 28.873348 , 29.42867  , 30.11124  , 30.919012 ,\n",
       "          30.2908   , 30.877293 , 30.623606 , 31.723726 , 32.62584  ,\n",
       "          32.156883 , 32.145863 , 32.08568  , 32.12507  , 30.233706 ,\n",
       "          31.256123 , 29.642857 , 30.489922 , 27.976486 , 28.370348 ,\n",
       "          29.065554 , 27.136566 , 28.586424 , 30.631649 , 31.50462  ,\n",
       "          32.025738 , 32.114616 , 32.210636 , 32.140903 , 30.921831 ,\n",
       "          31.137455 , 31.053627 , 30.380733 , 30.206448 , 30.584906 ,\n",
       "          30.054352 , 30.343832 , 28.48287  , 28.451479 , 24.675198 ,\n",
       "          21.146889 , 24.696201 , 17.433626 , 26.086557 , 27.301338 ,\n",
       "          27.265387 , 29.282425 , 30.623447 , 29.146275 , 27.52272  ,\n",
       "          27.98941  , 26.849022 , 26.740168 , 22.38552  , 26.839417 ,\n",
       "          25.774244 , 29.045444 , 27.960386 , 28.351294 , 26.508701 ,\n",
       "          29.440002 , 30.05442  , 27.924397 , 27.73619  , 27.995207 ,\n",
       "          20.528921 , 20.765812 , 20.544168 , 28.832373 , 29.41216  ,\n",
       "          24.71784  , 29.136946 , 27.31434  , 25.516087 , 28.598337 ,\n",
       "          28.874022 , 28.856686 , 29.12458  , 26.513725 , 26.403013 ,\n",
       "          28.738852 , 30.380236 , 31.10622  , 30.421041 , 30.409613 ,\n",
       "          27.905996 , 28.16787  , 28.562185 , 29.52323  , 32.14226  ,\n",
       "          30.318306 , 28.027819 , 26.413975 , 29.565416 , 28.970282 ,\n",
       "          29.826061 , 29.512224 , 31.537811 , 32.631557 , 31.484304 ,\n",
       "          31.545141 , 28.204159 , 28.776415 , 31.095438 , 29.054096 ,\n",
       "          29.708689 , 25.235985 , 26.537136 , 27.516718 , 28.378603 ,\n",
       "          28.342533 , 28.741978 , 27.157059 , 28.862612 , 29.119162 ,\n",
       "          30.469688 , 29.117876 , 30.994406 , 29.255133 , 29.341637 ,\n",
       "          27.652409 , 27.545116 , 20.736565 , 27.372335 , 26.40428  ,\n",
       "          25.29222  , 28.8366   , 27.054873 , 27.304193 , 25.882988 ,\n",
       "          25.884678 , 25.54018  , 28.416327 , 28.001759 , 28.558659 ,\n",
       "          29.993645 , 29.379341 , 28.49898  , 28.465815 , 29.212906 ,\n",
       "          30.855318 , 30.34043  , 29.961828 , 29.639576 , 29.473892 ,\n",
       "          26.471424 , 29.210135 , 30.053345 , 30.727957 , 32.94761  ,\n",
       "          32.15546  , 31.193436 , 33.843246 , 28.561438 , 26.848959 ,\n",
       "          29.268343 , 30.703053 , 28.9061   , 29.35433  , 28.848923 ,\n",
       "          30.778301 , 28.436895 , 30.09379  , 31.942581 , 32.588036 ,\n",
       "          33.461864 , 34.643566 , 33.95859  , 33.534912 , 31.044155 ,\n",
       "          31.758682 , 31.487186 , 29.986835 , 32.09246  , 32.108078 ,\n",
       "          30.08011  , 32.725605 , 33.202538 , 35.461895 , 34.702103 ,\n",
       "          36.060993 , 37.93341  , 37.990437 , 38.69841  , 36.546772 ,\n",
       "          38.402966 , 37.67462  , 40.327503 , 38.808056 , 38.59757  ,\n",
       "          37.354355 , 40.097652 , 39.22136  , 40.11081  , 42.41455  ,\n",
       "          41.79919  , 38.72615  , 36.235508 , 38.690426 , 37.238087 ,\n",
       "          36.860565 , 37.63107  , 38.600697 , 38.1995   , 38.342266 ,\n",
       "          38.040173 , 38.063576 , 38.05651  , 38.64399  , 40.61821  ,\n",
       "          38.614876 , 42.18752  , 39.919403 , 39.836254 , 40.241505 ,\n",
       "          42.736107 , 39.877953 , 40.51393  , 41.508724 , 40.397    ,\n",
       "          40.910168 , 40.498013 , 40.27202  , 39.93956  , 39.096313 ,\n",
       "          38.720898 , 38.018013 , 39.356915 , 39.80988  , 39.0107   ,\n",
       "          38.62855  , 39.78748  , 39.326687 , 38.84446  , 41.0381   ,\n",
       "          40.05979  , 42.211327 , 40.55543  , 38.72581  , 35.50433  ,\n",
       "          33.15156  , 36.467445 , 34.798786 , 35.143898 , 35.689423 ,\n",
       "          35.015102 , 36.370106 , 36.476543 , 36.59173  , 34.445282 ,\n",
       "          33.071697 , 31.722181 , 36.62848  , 35.782845 , 34.39219  ,\n",
       "          28.274609 , 27.277254 , 25.39128  , 12.862366 ,  4.698904 ,\n",
       "           9.863125 , 29.90889  , 19.838423 , 21.137693 , 33.227917 ,\n",
       "          32.970398 , 26.007902 , 29.414219 , 33.386658 , 32.39855  ,\n",
       "          16.183578 , 28.62634  , 36.90141  , 36.811344 , 36.869556 ,\n",
       "          39.44994  , 39.804955 , 38.48359  , 40.009052 , 39.709885 ,\n",
       "          43.02932  , 41.918503 , 44.804497 , 43.032177 , 41.58474  ,\n",
       "          43.969055 , 45.815964 , 46.111465 , 47.253162 , 48.00756  ,\n",
       "          46.111145 , 47.686134 , 46.296547 , 47.267548 , 47.958515 ,\n",
       "          42.02845  , 46.445835 , 45.28598  , 45.598274 , 42.70627  ,\n",
       "          43.30676  , 42.91156  , 38.901825 , 38.331062 , 41.85373  ,\n",
       "          44.1439   , 44.823734 , 45.402973 , 46.05648  , 43.599392 ,\n",
       "          45.608562 , 46.367874 , 47.931347 , 50.099613 , 51.423164 ,\n",
       "          51.921333 , 51.116806 , 48.533573 , 49.353764 , 51.01707  ,\n",
       "          50.999355 , 52.101753 , 52.731388 , 52.267128 , 53.069786 ,\n",
       "          53.526093 , 54.40946  , 52.984356 , 52.713955 , 51.782814 ,\n",
       "          52.99036  , 53.61709  , 53.616722 , 54.582867 , 55.174137 ,\n",
       "          55.692905 , 55.687843 , 53.244335 , 52.042747 , 51.7111   ,\n",
       "          54.247387 , 53.975327 , 54.110504 , 54.86047  , 54.442204 ,\n",
       "          54.59453  , 50.847523 , 52.842854 , 54.923733 , 53.203354 ,\n",
       "          52.044605 , 48.57166  , 50.018295 , 50.36679  , 50.743805 ,\n",
       "          51.15432  , 51.724545 , 51.65057  , 49.591064 , 49.721752 ,\n",
       "          44.74381  , 49.513412 , 49.29022  , 51.188946 , 49.48531  ,\n",
       "          51.719593 , 50.495228 , 50.902714 , 51.452557 , 51.513752 ,\n",
       "          56.99558  , 53.626606 , 55.109184 , 57.32006  , 56.420982 ,\n",
       "          55.64347  , 55.371906 , 58.149998 , 58.330627 , 55.8005   ,\n",
       "          54.714012 , 55.281902 , 54.717293 , 55.871803 , 52.709415 ,\n",
       "          51.4374   , 54.2903   , 51.54014  , 50.73443  , 49.893337 ,\n",
       "          50.198578 , 45.737324 , 40.384453 , 43.900368 ,  4.19298  ,\n",
       "          21.630276 , 31.28498  , 40.02079  , 34.719013 , 22.665482 ,\n",
       "          30.285952 , 42.299335 , 20.716028 , 42.98018  , 43.155254 ,\n",
       "          43.601974 , 45.674557 , 44.030983 , 46.29506  , 46.5326   ,\n",
       "          48.242615 , 45.327084 , 42.744022 , 42.876106 , 43.25196  ,\n",
       "          40.296913 , 20.625963 ,  5.4051633, 40.82934  , 41.9999   ,\n",
       "          42.35335  , 41.01998  , 41.718838 , 41.781094 , 36.02886  ,\n",
       "          35.426888 , 30.570454 , 41.61625  , 45.09385  , 43.553562 ,\n",
       "          45.386765 , 41.580982 , 40.024704 , 40.175323 , 41.487953 ,\n",
       "          42.785683 , 43.610043 , 42.59862  , 43.141163 , 44.079082 ,\n",
       "          45.06972  , 44.559586 , 44.769234 , 43.62031  , 40.80087  ,\n",
       "          39.618958 , 43.246197 , 44.907097 , 46.330902 , 45.865074 ,\n",
       "          44.80873  , 46.673355 , 45.473827 , 44.896606 , 45.874157 ,\n",
       "          48.74119  , 50.453503 , 48.681767 , 49.592327 , 48.882355 ,\n",
       "          50.794422 , 50.980145 , 44.191353 , 43.60242  , 44.356518 ,\n",
       "          44.391727 , 45.00895  , 46.265785 , 46.24252  , 47.741894 ,\n",
       "          47.641434 , 44.845787 , 45.413986 , 44.862644 , 45.823215 ,\n",
       "          50.0524   , 50.28477  , 50.6477   , 52.38364  , 47.75348  ,\n",
       "          51.431583 , 54.140472 , 50.58671  , 52.07413  , 49.381317 ,\n",
       "          50.11864  , 52.503525 , 54.325073 ], dtype=float32),\n",
       "   'run_time': 2.133913516998291,\n",
       "   'status': 'ok'},\n",
       "  'misc': {'tid': 1,\n",
       "   'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "   'workdir': None,\n",
       "   'idxs': {'activation': [1],\n",
       "    'batch_normalization': [1],\n",
       "    'batch_size': [1],\n",
       "    'complete_inputs': [1],\n",
       "    'complete_sample': [1],\n",
       "    'device': [1],\n",
       "    'dropout_prob_exogenous': [1],\n",
       "    'dropout_prob_theta': [1],\n",
       "    'early_stop_patience': [1],\n",
       "    'eval_freq': [1],\n",
       "    'frequency': [1],\n",
       "    'idx_to_sample_freq': [1],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [1],\n",
       "    'learning_rate': [1],\n",
       "    'len_sample_chunks': [1],\n",
       "    'loss': [1],\n",
       "    'loss_hypar': [1],\n",
       "    'loss_valid': [1],\n",
       "    'lr_decay': [1],\n",
       "    'lr_decay_step_size': [1],\n",
       "    'max_epochs': [1],\n",
       "    'max_steps': [1],\n",
       "    'n_blocks': [1],\n",
       "    'n_harmonics': [1],\n",
       "    'n_hidden': [1],\n",
       "    'n_layers': [1],\n",
       "    'n_polynomials': [1],\n",
       "    'n_s_hidden': [1],\n",
       "    'n_series_per_batch': [1],\n",
       "    'n_time_in': [1],\n",
       "    'n_time_out': [1],\n",
       "    'n_val_weeks': [1],\n",
       "    'n_x_hidden': [1],\n",
       "    'normalizer_x': [1],\n",
       "    'normalizer_y': [1],\n",
       "    'random_seed': [1],\n",
       "    'seasonality': [1],\n",
       "    'shared_weights': [1],\n",
       "    'stack_types': [1],\n",
       "    'val_idx_to_sample_freq': [1],\n",
       "    'weight_decay': [1],\n",
       "    'window_sampling_limit': [1]},\n",
       "   'vals': {'activation': [0],\n",
       "    'batch_normalization': [0],\n",
       "    'batch_size': [0],\n",
       "    'complete_inputs': [0],\n",
       "    'complete_sample': [0],\n",
       "    'device': [0],\n",
       "    'dropout_prob_exogenous': [0.36553112690476813],\n",
       "    'dropout_prob_theta': [0.24107333575459605],\n",
       "    'early_stop_patience': [0],\n",
       "    'eval_freq': [0],\n",
       "    'frequency': [0],\n",
       "    'idx_to_sample_freq': [0],\n",
       "    'initialization': [1],\n",
       "    'l1_theta': [0],\n",
       "    'learning_rate': [0.0008532112183090035],\n",
       "    'len_sample_chunks': [0],\n",
       "    'loss': [0],\n",
       "    'loss_hypar': [0],\n",
       "    'loss_valid': [0],\n",
       "    'lr_decay': [0.3553227609462469],\n",
       "    'lr_decay_step_size': [0],\n",
       "    'max_epochs': [0],\n",
       "    'max_steps': [0],\n",
       "    'n_blocks': [0],\n",
       "    'n_harmonics': [0],\n",
       "    'n_hidden': [0],\n",
       "    'n_layers': [0],\n",
       "    'n_polynomials': [0],\n",
       "    'n_s_hidden': [0],\n",
       "    'n_series_per_batch': [0],\n",
       "    'n_time_in': [0],\n",
       "    'n_time_out': [0],\n",
       "    'n_val_weeks': [0],\n",
       "    'n_x_hidden': [6.0],\n",
       "    'normalizer_x': [0],\n",
       "    'normalizer_y': [0],\n",
       "    'random_seed': [18.0],\n",
       "    'seasonality': [0],\n",
       "    'shared_weights': [0],\n",
       "    'stack_types': [0],\n",
       "    'val_idx_to_sample_freq': [0],\n",
       "    'weight_decay': [0.00012349475141123045],\n",
       "    'window_sampling_limit': [0]}},\n",
       "  'exp_key': None,\n",
       "  'owner': None,\n",
       "  'version': 0,\n",
       "  'book_time': datetime.datetime(2021, 6, 16, 16, 19, 29, 151000),\n",
       "  'refresh_time': datetime.datetime(2021, 6, 16, 16, 19, 31, 305000)}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nixtla] *",
   "language": "python",
   "name": "conda-env-nixtla-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
