---

title: Experiment Utils


keywords: fastai
sidebar: home_sidebar

summary: "Set of functions to easily perform experiments."
description: "Set of functions to easily perform experiments."
nb_path: "nbs/experiments__utils.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/experiments__utils.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_mask_dfs" class="doc_header"><code>get_mask_dfs</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L36" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_mask_dfs</code>(<strong><code>Y_df</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>ds_in_test</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_random_mask_dfs" class="doc_header"><code>get_random_mask_dfs</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L66" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_random_mask_dfs</code>(<strong><code>Y_df</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>n_ds_val_window</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>freq</code></strong>)</p>
</blockquote>
<p>Generates train, test and random validation mask.
Train mask begins by avoiding ds_in_test</p>
<p>Validation mask: 1) samples n_uids unique ids
                 2) creates windows of size n_ds_val_window</p>
<h2 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h2><p>ds_in_test: int
    Number of ds in test.
n_uids: int
    Number of unique ids in validation.
n_val_windows: int
    Number of windows for validation.
n_ds_val_window: int
    Number of ds in each validation window.
periods: int
    ds_in_test multiplier.
freq: str
    string that determines datestamp frequency, used in
    random windows creation.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="scale_data" class="doc_header"><code>scale_data</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L122" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>scale_data</code>(<strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>mask_df</code></strong>, <strong><code>normalizer_y</code></strong>, <strong><code>normalizer_x</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_datasets" class="doc_header"><code>create_datasets</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L140" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_datasets</code>(<strong><code>mc</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_loaders" class="doc_header"><code>instantiate_loaders</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L210" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_loaders</code>(<strong><code>mc</code></strong>, <strong><code>train_dataset</code></strong>, <strong><code>val_dataset</code></strong>, <strong><code>test_dataset</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_nbeats" class="doc_header"><code>instantiate_nbeats</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L233" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_nbeats</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_esrnn" class="doc_header"><code>instantiate_esrnn</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L265" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_esrnn</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_mqesrnn" class="doc_header"><code>instantiate_mqesrnn</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L296" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_mqesrnn</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="instantiate_model" class="doc_header"><code>instantiate_model</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L324" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>instantiate_model</code>(<strong><code>mc</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="model_fit_predict" class="doc_header"><code>model_fit_predict</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L331" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>model_fit_predict</code>(<strong><code>mc</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="evaluate_model" class="doc_header"><code>evaluate_model</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L400" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>evaluate_model</code>(<strong><code>mc</code></strong>, <strong><code>loss_function</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_test</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>, <strong><code>loss_kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="hyperopt_tunning" class="doc_header"><code>hyperopt_tunning</code><a href="https://github.com/Nixtla/nixtlats/tree/master/nixtlats/experiments/utils.py#L446" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>hyperopt_tunning</code>(<strong><code>space</code></strong>, <strong><code>hyperopt_max_evals</code></strong>, <strong><code>loss_function</code></strong>, <strong><code>S_df</code></strong>, <strong><code>Y_df</code></strong>, <strong><code>X_df</code></strong>, <strong><code>f_cols</code></strong>, <strong><code>ds_in_val</code></strong>, <strong><code>n_uids</code></strong>, <strong><code>n_val_windows</code></strong>, <strong><code>freq</code></strong>, <strong><code>is_val_random</code></strong>, <strong><code>save_trials</code></strong>=<em><code>False</code></em>, <strong><code>loss_kwargs</code></strong>=<em><code>None</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Experiment-Utils-Examples">Experiment Utils Examples<a class="anchor-link" href="#Experiment-Utils-Examples"> </a></h1>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
<span class="kn">from</span> <span class="nn">nixtlats.losses.numpy</span> <span class="kn">import</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mape</span><span class="p">,</span> <span class="n">smape</span><span class="p">,</span> <span class="n">rmse</span><span class="p">,</span> <span class="n">pinball_loss</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span>
<span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span> <span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span>  

<span class="n">nbeats_space</span><span class="o">=</span> <span class="p">{</span><span class="c1"># Architecture parameters</span>
               <span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;nbeats&#39;</span><span class="p">,</span>
               <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;simple&#39;</span><span class="p">,</span>
               <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_in&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_out&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_x_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;n_x_hidden&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;n_s_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_s_hidden&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
               <span class="s1">&#39;shared_weights&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;shared_weights&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;activation&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;SELU&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;initialization&#39;</span><span class="p">:</span>  <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;initialization&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;glorot_normal&#39;</span><span class="p">,</span><span class="s1">&#39;he_normal&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;stack_types&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;stack_types&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">],</span>
                                                        <span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;exogenous_tcn&#39;</span><span class="p">],</span>
                                                        <span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;exogenous_tcn&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_blocks&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_layers&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="p">]),</span>
               <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_hidden&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="mi">256</span> <span class="p">]),</span>
               <span class="s1">&#39;n_harmonics&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_harmonics&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
               <span class="s1">&#39;n_polynomials&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_polynomials&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
               <span class="c1"># Regularization and optimization parameters</span>
               <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_normalization&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;dropout_prob_exogenous&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;dropout_prob_exogenous&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
               <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="s1">&#39;lr_decay&#39;</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
               <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">]),</span> 
               <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-5</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)),</span>
               <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span> <span class="c1">#&#39;n_iterations&#39;: hp.choice(&#39;n_iterations&#39;, [10])</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_steps&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;early_stop_patience&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">16</span><span class="p">]),</span>
               <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;eval_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">]),</span>
               <span class="s1">&#39;n_val_weeks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_val_weeks&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">52</span><span class="o">*</span><span class="mi">2</span><span class="p">]),</span>
               <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;loss_hypar&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_hypar&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]),</span>                
               <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_valid&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span> <span class="c1">#[args.val_loss]),</span>
               <span class="s1">&#39;l1_theta&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;l1_theta&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
               <span class="c1"># Data parameters</span>
               <span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_y&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_x&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;median&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100_000</span><span class="p">]),</span>
               <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_inputs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;complete_sample&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_sample&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">False</span><span class="p">]),</span>                
               <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;frequency&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;H&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;seasonality&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>      
               <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">]),</span>
               <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
               <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;random_seed&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;device&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">device</span><span class="p">])}</span>

<span class="n">mc</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;nbeats&#39;</span><span class="p">,</span>
      <span class="c1"># Architecture parameters</span>
      <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_x_hidden&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
      <span class="s1">&#39;n_s_hidden&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="s1">&#39;shared_weights&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="s1">&#39;SELU&#39;</span><span class="p">,</span>
      <span class="s1">&#39;initialization&#39;</span><span class="p">:</span> <span class="s1">&#39;he_normal&#39;</span><span class="p">,</span>
      <span class="s1">&#39;stack_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;exogenous_tcn&#39;</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;identity&#39;</span><span class="p">],</span>
      <span class="s1">&#39;n_blocks&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
      <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
      <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="mi">364</span><span class="p">,</span>
      <span class="s1">&#39;n_polynomials&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
      <span class="s1">&#39;n_harmonics&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="c1"># Regularization and optimization parameters</span>
      <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="c1">#&#39;n_iterations&#39;: 100,</span>
      <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>      
      <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
      <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;dropout_prob_theta&#39;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
      <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="c1">#0.002,</span>
      <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="mf">0.64</span><span class="p">,</span>
      <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
      <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.00015</span><span class="p">,</span>
      <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;n_val_weeks&#39;</span><span class="p">:</span> <span class="mi">52</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span>
      <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="s1">&#39;PINBALL&#39;</span><span class="p">,</span>
      <span class="s1">&#39;loss_hypar&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="c1">#0.49,</span>
      <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="s1">&#39;MAE&#39;</span><span class="p">,</span>
      <span class="s1">&#39;l1_theta&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="c1"># Data parameters</span>
      <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
      <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="s1">&#39;median&#39;</span><span class="p">,</span>
      <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="mi">100_000</span><span class="p">,</span>
      <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;frequency&#39;</span><span class="p">:</span><span class="s1">&#39;H&#39;</span><span class="p">,</span>
      <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>
      <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;device&#39;</span><span class="p">:</span> <span class="s1">&#39;cpu&#39;</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">esrnn_space</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;esrnn&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span>
               <span class="c1"># Architecture parameters</span>
               <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_in&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_time_out&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;dilations&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]]</span> <span class="p">]),</span>
               <span class="s1">&#39;es_component&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;es_component&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;multiplicative&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;cell_type&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;cell_type&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LSTM&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;state_hsize&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;state_hsize&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
               <span class="s1">&#39;add_nl_layer&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;add_nl_layer&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span>
               <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;seasonality&#39;</span><span class="p">,</span> <span class="p">[</span> <span class="p">[</span><span class="mi">24</span><span class="p">]</span> <span class="p">]),</span>
               <span class="c1"># Regularization and optimization parameters</span>
               <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_epochs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span>
               <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;max_steps&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span><span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;early_stop_patience&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span>
               <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;eval_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">]),</span>
               <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;batch_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">32</span><span class="p">]),</span>
               <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">5e-4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)),</span>
               <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;lr_decay&#39;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
               <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">]),</span> 
               <span class="s1">&#39;per_series_lr_multip&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;per_series_lr_multip&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span>
               <span class="s1">&#39;gradient_eps&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;gradient_eps&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1e-8</span><span class="p">]),</span>
               <span class="s1">&#39;gradient_clipping_threshold&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;gradient_clipping_threshold&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span>
               <span class="s1">&#39;rnn_weight_decay&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;rnn_weight_decay&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.0005</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">]),</span>
               <span class="s1">&#39;noise_std&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="s1">&#39;noise_std&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
               <span class="s1">&#39;level_variability_penalty&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;level_variability_penalty&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
               <span class="s1">&#39;testing_percentile&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;testing_percentile&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">50</span><span class="p">]),</span>
               <span class="s1">&#39;training_percentile&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;training_percentile&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">48</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">]),</span>
               <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">quniform</span><span class="p">(</span><span class="s1">&#39;random_seed&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
               <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_train&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;SMYL&#39;</span><span class="p">]),</span>
               <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;loss_valid&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]),</span>
               <span class="c1"># Data parameters</span>
               <span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">7</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">500_000</span><span class="p">]),</span>
               <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_inputs&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
               <span class="s1">&#39;complete_sample&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;complete_sample&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">True</span><span class="p">]),</span>
               <span class="s1">&#39;sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;val_sample_freq&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;val_sample_freq&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">24</span><span class="p">]),</span>
               <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_y&#39;</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">]),</span>
               <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="n">hp</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="s1">&#39;normalizer_x&#39;</span><span class="p">,</span>  <span class="p">[</span><span class="kc">None</span><span class="p">])}</span>

<span class="n">mc</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span><span class="s1">&#39;esrnn&#39;</span><span class="p">,</span>
      <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span>
      <span class="c1"># Architecture parameters</span>
      <span class="s1">&#39;n_series&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;n_time_in&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_time_out&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_x&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;n_s&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;dilations&#39;</span><span class="p">:</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">]],</span>
      <span class="s1">&#39;es_component&#39;</span><span class="p">:</span> <span class="s1">&#39;multiplicative&#39;</span><span class="p">,</span>
      <span class="s1">&#39;cell_type&#39;</span><span class="p">:</span> <span class="s1">&#39;LSTM&#39;</span><span class="p">,</span>
      <span class="s1">&#39;state_hsize&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;add_nl_layer&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;seasonality&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">24</span><span class="p">],</span>
      <span class="c1"># Regularization and optimization parameters</span>
      <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="c1">#&#39;n_iterations&#39;: 100,</span>
      <span class="s1">&#39;max_steps&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
      <span class="s1">&#39;early_stop_patience&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;eval_freq&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
      <span class="s1">&#39;eq_batch_size&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
      <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.0005</span><span class="p">,</span>
      <span class="s1">&#39;lr_decay&#39;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span>
      <span class="s1">&#39;lr_decay_step_size&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
      <span class="s1">&#39;per_series_lr_multip&#39;</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span>
      <span class="s1">&#39;gradient_eps&#39;</span><span class="p">:</span> <span class="mf">1e-8</span><span class="p">,</span> 
      <span class="s1">&#39;gradient_clipping_threshold&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
      <span class="s1">&#39;rnn_weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
      <span class="s1">&#39;noise_std&#39;</span><span class="p">:</span> <span class="mf">0.0005</span><span class="p">,</span>
      <span class="s1">&#39;level_variability_penalty&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
      <span class="s1">&#39;testing_percentile&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;training_percentile&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
      <span class="s1">&#39;random_seed&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;loss_train&#39;</span><span class="p">:</span> <span class="s1">&#39;SMYL&#39;</span><span class="p">,</span>
      <span class="s1">&#39;loss_valid&#39;</span><span class="p">:</span> <span class="s1">&#39;MAE&#39;</span><span class="p">,</span>
      <span class="c1"># Data parameters</span>
      <span class="s1">&#39;len_sample_chunks&#39;</span><span class="p">:</span> <span class="mi">7</span><span class="o">*</span><span class="mi">4</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;window_sampling_limit&#39;</span><span class="p">:</span> <span class="mi">500_000</span><span class="p">,</span>
      <span class="s1">&#39;complete_inputs&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
      <span class="s1">&#39;sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;val_idx_to_sample_freq&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
      <span class="s1">&#39;n_series_per_batch&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s1">&#39;normalizer_y&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
      <span class="s1">&#39;normalizer_x&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">instantiate_esrnn</span><span class="p">(</span><span class="n">mc</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nixtlats.data.datasets.epf</span> <span class="kn">import</span> <span class="n">EPF</span><span class="p">,</span> <span class="n">EPFInfo</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NP&#39;</span><span class="p">]</span>

<span class="n">Y_df</span><span class="p">,</span> <span class="n">X_df</span><span class="p">,</span> <span class="n">S_df</span> <span class="o">=</span> <span class="n">EPF</span><span class="o">.</span><span class="n">load_groups</span><span class="p">(</span><span class="n">directory</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">dataset</span><span class="p">)</span>

<span class="n">X_df</span> <span class="o">=</span> <span class="n">X_df</span><span class="p">[[</span><span class="s1">&#39;unique_id&#39;</span><span class="p">,</span> <span class="s1">&#39;ds&#39;</span><span class="p">,</span> <span class="s1">&#39;week_day&#39;</span><span class="p">]]</span>
<span class="n">Y_min</span> <span class="o">=</span> <span class="n">Y_df</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="c1">#Y_df.y = Y_df.y - Y_min + 20</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Y_df</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># backpropagation trough time is slow</span>
<span class="c1"># result = evaluate_model(loss_function=mae, mc=mc, </span>
<span class="c1">#                         S_df=S_df, Y_df=Y_df, X_df=X_df, f_cols=[],</span>
<span class="c1">#                         ds_in_test=0, ds_in_val=728*24,</span>
<span class="c1">#                         n_uids=None, n_val_windows=None, freq=None,</span>
<span class="c1">#                         is_val_random=False, loss_kwargs={})</span>
<span class="c1"># result</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># plt.plot(Y_df[&#39;y&#39;][-728*24:].values)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trials</span> <span class="o">=</span> <span class="n">hyperopt_tunning</span><span class="p">(</span><span class="n">space</span><span class="o">=</span><span class="n">nbeats_space</span><span class="p">,</span> <span class="n">hyperopt_max_evals</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loss_function</span><span class="o">=</span><span class="n">mae</span><span class="p">,</span>
                          <span class="n">S_df</span><span class="o">=</span><span class="n">S_df</span><span class="p">,</span> <span class="n">Y_df</span><span class="o">=</span><span class="n">Y_df</span><span class="p">,</span> <span class="n">X_df</span><span class="o">=</span><span class="n">X_df</span><span class="p">,</span> <span class="n">f_cols</span><span class="o">=</span><span class="p">[],</span>
                          <span class="n">ds_in_val</span><span class="o">=</span><span class="mi">728</span><span class="o">*</span><span class="mi">24</span><span class="p">,</span> <span class="n">n_uids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_val_windows</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">is_val_random</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loss_kwargs</span><span class="o">=</span><span class="p">{})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:hyperopt.tpe:build_posterior_wrapper took 0.012577 seconds
INFO:hyperopt.tpe:TPE using 0 trials
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>===============================================      

activation                                     SELU  
batch_normalization                           False
batch_size                                      256
complete_inputs                               False
complete_sample                               False
device                                         cuda
dropout_prob_exogenous                     0.246426
dropout_prob_theta                         0.044532
early_stop_patience                              16
eval_freq                                        50
frequency                                         H
idx_to_sample_freq                               24
initialization                            he_normal
l1_theta                                          0
learning_rate                               0.00059
len_sample_chunks                              None
loss_hypar                                      0.5
loss_train                                      MAE
loss_valid                                      MAE
lr_decay                                   0.472903
lr_decay_step_size                              100
max_epochs                                       10
max_steps                                      None
mode                                         simple
model                                        nbeats
n_blocks                                     (1, 1)
n_harmonics                                       1
n_hidden                                        256
n_layers                                     (2, 2)
n_polynomials                                     2
n_s_hidden                                        0
n_series_per_batch                                1
n_time_in                                       168
n_time_out                                       24
n_val_weeks                                     104
n_x_hidden                                      5.0
normalizer_x                                 median
normalizer_y                                   None
random_seed                                    16.0
seasonality                                      24
shared_weights                                False
stack_types               (exogenous_tcn, identity)
val_idx_to_sample_freq                           24
weight_decay                               0.000058
window_sampling_limit                        100000
dtype: object
===============================================      

  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2016-12-27 2018-12-24 23:00:00
          1           2013-01-01 2016-12-26 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=66.67, 	34944 time stamps 
Outsample percentage=33.33, 	17472 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2016-12-26 23:00:00
          1           2016-12-27 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=33.33, 	17472 time stamps 
Outsample percentage=66.67, 	34944 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.0, 	0 time stamps 
Outsample percentage=100.0, 	52416 time stamps 

GPU available: True, used: False
TPU available: False, using: 0 TPU cores
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  warnings.warn(*args, **kwargs)


  | Name  | Type    | Params
----------------------------------
0 | model | _NBEATS | 368 K 
----------------------------------
368 K     Trainable params
0         Non-trainable params
368 K     Total params
1.475     Total estimated model params size (MB)
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

Metric val_loss improved. New best score: 12.557
Metric val_loss improved by 6.057 &gt;= min_delta = 0.0001. New best score: 6.500
Metric val_loss improved by 1.609 &gt;= min_delta = 0.0001. New best score: 4.891
Metric val_loss improved by 0.366 &gt;= min_delta = 0.0001. New best score: 4.525
Metric val_loss improved by 1.341 &gt;= min_delta = 0.0001. New best score: 3.184
Metric val_loss improved by 0.115 &gt;= min_delta = 0.0001. New best score: 3.069
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>y_true.shape (#n_series, #n_fcds, #lt): (728,)       
y_hat.shape (#n_series, #n_fcds, #lt): (728,)        
 50%|█████     | 1/2 [00:02&lt;00:02,  2.67s/trial, best loss: 3.701502561569214]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:hyperopt.tpe:build_posterior_wrapper took 0.013880 seconds
INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 3.701503
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>===============================================                               

activation                                SELU                                
batch_normalization                      False
batch_size                                 256
complete_inputs                          False
complete_sample                          False
device                                    cuda
dropout_prob_exogenous                0.365531
dropout_prob_theta                    0.241073
early_stop_patience                         16
eval_freq                                   50
frequency                                    H
idx_to_sample_freq                          24
initialization                       he_normal
l1_theta                                     0
learning_rate                         0.000853
len_sample_chunks                         None
loss_hypar                                 0.5
loss_train                                 MAE
loss_valid                                 MAE
lr_decay                              0.355323
lr_decay_step_size                         100
max_epochs                                  10
max_steps                                 None
mode                                    simple
model                                   nbeats
n_blocks                                (1, 1)
n_harmonics                                  1
n_hidden                                   256
n_layers                                (2, 2)
n_polynomials                                2
n_s_hidden                                   0
n_series_per_batch                           1
n_time_in                                  168
n_time_out                                  24
n_val_weeks                                104
n_x_hidden                                 6.0
normalizer_x                            median
normalizer_y                              None
random_seed                               18.0
seasonality                                 24
shared_weights                           False
stack_types               (identity, identity)
val_idx_to_sample_freq                      24
weight_decay                          0.000123
window_sampling_limit                   100000
dtype: object
===============================================                               

 50%|█████     | 1/2 [00:02&lt;00:02,  2.67s/trial, best loss: 3.701502561569214]</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2016-12-27 2018-12-24 23:00:00
          1           2013-01-01 2016-12-26 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=66.67, 	34944 time stamps 
Outsample percentage=33.33, 	17472 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2016-12-26 23:00:00
          1           2016-12-27 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=33.33, 	17472 time stamps 
Outsample percentage=66.67, 	34944 time stamps 

INFO:root:Train Validation splits

INFO:root:                              ds                    
                             min                 max
unique_id sample_mask                               
NP        0           2013-01-01 2018-12-24 23:00:00
INFO:root:
Total data 			52416 time stamps 
Available percentage=100.0, 	52416 time stamps 
Insample  percentage=0.0, 	0 time stamps 
Outsample percentage=100.0, 	52416 time stamps 

GPU available: True, used: False
TPU available: False, using: 0 TPU cores
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  warnings.warn(*args, **kwargs)


  | Name  | Type    | Params
----------------------------------
0 | model | _NBEATS | 415 K 
----------------------------------
415 K     Trainable params
0         Non-trainable params
415 K     Total params
1.660     Total estimated model params size (MB)
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

Metric val_loss improved. New best score: 4.409
Metric val_loss improved by 0.711 &gt;= min_delta = 0.0001. New best score: 3.698
Metric val_loss improved by 0.311 &gt;= min_delta = 0.0001. New best score: 3.388
Metric val_loss improved by 0.352 &gt;= min_delta = 0.0001. New best score: 3.035
Metric val_loss improved by 0.003 &gt;= min_delta = 0.0001. New best score: 3.032
Metric val_loss improved by 0.147 &gt;= min_delta = 0.0001. New best score: 2.885
Metric val_loss improved by 0.074 &gt;= min_delta = 0.0001. New best score: 2.811
/home/ubuntu/anaconda3/envs/nixtla/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, predict dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)

</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>y_true.shape (#n_series, #n_fcds, #lt): (728,)                                
y_hat.shape (#n_series, #n_fcds, #lt): (728,)                                 
100%|██████████| 2/2 [00:04&lt;00:00,  2.42s/trial, best loss: 1.9704331159591675]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">trials</span><span class="o">.</span><span class="n">trials</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[{&#39;state&#39;: 2,
  &#39;tid&#39;: 0,
  &#39;spec&#39;: None,
  &#39;result&#39;: {&#39;loss&#39;: 3.701502561569214,
   &#39;mc&#39;: {&#39;activation&#39;: &#39;SELU&#39;,
    &#39;batch_normalization&#39;: False,
    &#39;batch_size&#39;: 256,
    &#39;complete_inputs&#39;: False,
    &#39;complete_sample&#39;: False,
    &#39;device&#39;: &#39;cuda&#39;,
    &#39;dropout_prob_exogenous&#39;: 0.24642550819005665,
    &#39;dropout_prob_theta&#39;: 0.04453225609606154,
    &#39;early_stop_patience&#39;: 16,
    &#39;eval_freq&#39;: 50,
    &#39;frequency&#39;: &#39;H&#39;,
    &#39;idx_to_sample_freq&#39;: 24,
    &#39;initialization&#39;: &#39;he_normal&#39;,
    &#39;l1_theta&#39;: 0,
    &#39;learning_rate&#39;: 0.0005903125740803596,
    &#39;len_sample_chunks&#39;: None,
    &#39;loss_hypar&#39;: 0.5,
    &#39;loss_train&#39;: &#39;MAE&#39;,
    &#39;loss_valid&#39;: &#39;MAE&#39;,
    &#39;lr_decay&#39;: 0.47290326767085267,
    &#39;lr_decay_step_size&#39;: 100,
    &#39;max_epochs&#39;: 10,
    &#39;max_steps&#39;: None,
    &#39;mode&#39;: &#39;simple&#39;,
    &#39;model&#39;: &#39;nbeats&#39;,
    &#39;n_blocks&#39;: (1, 1),
    &#39;n_harmonics&#39;: 1,
    &#39;n_hidden&#39;: 256,
    &#39;n_layers&#39;: (2, 2),
    &#39;n_polynomials&#39;: 2,
    &#39;n_s_hidden&#39;: 0,
    &#39;n_series_per_batch&#39;: 1,
    &#39;n_time_in&#39;: 168,
    &#39;n_time_out&#39;: 24,
    &#39;n_val_weeks&#39;: 104,
    &#39;n_x_hidden&#39;: 5.0,
    &#39;normalizer_x&#39;: &#39;median&#39;,
    &#39;normalizer_y&#39;: None,
    &#39;random_seed&#39;: 16.0,
    &#39;seasonality&#39;: 24,
    &#39;shared_weights&#39;: False,
    &#39;stack_types&#39;: (&#39;exogenous_tcn&#39;, &#39;identity&#39;),
    &#39;val_idx_to_sample_freq&#39;: 24,
    &#39;weight_decay&#39;: 5.8073579520205266e-05,
    &#39;window_sampling_limit&#39;: 100000,
    &#39;n_x&#39;: 1,
    &#39;n_s&#39;: 1,
    &#39;n_theta_hidden&#39;: [[256, 256], [256, 256]]},
   &#39;y_true&#39;: array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,
          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,
          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,
          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,
          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,
          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,
          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,
          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,
          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,
          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,
          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,
          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,
          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,
          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,
          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,
          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,
          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,
          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,
          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,
          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,
          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,
          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,
          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,
          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,
          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,
          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,
          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,
          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,
          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,
          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,
          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,
          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,
          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,
          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,
          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,
          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,
          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,
          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,
          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,
          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,
          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,
          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,
          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,
          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,
          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,
          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,
          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,
          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,
          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,
          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,
          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,
          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,
          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,
          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,
          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,
          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,
          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,
          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,
          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,
          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,
          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,
          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,
          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,
          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,
          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,
          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,
          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,
          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,
          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,
          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,
          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,
          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,
          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,
          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,
          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,
          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,
          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,
          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,
          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,
          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,
          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],
         dtype=float32),
   &#39;y_hat&#39;: array([27.701649 , 27.975748 , 31.803595 , 31.472857 , 28.643927 ,
          28.219172 , 31.853521 , 33.827473 , 30.845858 , 33.125107 ,
          36.181877 , 31.319834 , 33.202126 , 32.05909  , 31.78731  ,
          31.477406 , 27.625643 , 31.886206 , 33.18131  , 31.663424 ,
          33.47664  , 33.102306 , 30.76027  , 30.854624 , 30.513063 ,
          31.119108 , 31.45578  , 30.727148 , 31.015617 , 31.072279 ,
          29.91333  , 31.884443 , 31.577305 , 31.11768  , 30.809654 ,
          31.933662 , 32.662987 , 31.777435 , 33.141994 , 33.583065 ,
          33.415916 , 33.67277  , 33.67393  , 34.7538   , 35.73098  ,
          34.790936 , 33.86699  , 33.681534 , 33.691025 , 35.487526 ,
          35.726833 , 34.177856 , 33.04427  , 33.61656  , 30.676077 ,
          32.29714  , 30.447182 , 32.80938  , 29.365845 , 30.978231 ,
          33.46982  , 31.01993  , 31.37819  , 30.004261 , 32.07784  ,
          33.05431  , 32.186184 , 34.075848 , 31.757715 , 33.468113 ,
          33.654778 , 34.731728 , 32.142467 , 33.155827 , 34.23377  ,
          32.650238 , 32.461346 , 32.81042  , 27.75075  , 31.551151 ,
          30.965195 , 28.564816 , 31.68756  , 29.758919 , 29.686258 ,
          30.2022   , 31.718147 , 32.293495 , 31.605698 , 30.203241 ,
          30.906057 , 31.021599 , 31.610073 , 32.170662 , 30.912388 ,
          30.811941 , 29.598139 , 30.292961 , 30.65223  , 29.147573 ,
          27.314524 , 29.550224 , 29.97744  , 29.968029 , 28.101295 ,
          25.158245 , 27.554085 , 27.501097 , 30.007065 , 31.786043 ,
          29.360703 , 31.8217   , 31.33803  , 32.28236  , 32.072117 ,
          29.792221 , 27.287334 , 28.04121  , 32.11424  , 31.66252  ,
          33.691414 , 33.059284 , 33.28169  , 32.11442  , 31.789    ,
          30.190199 , 29.862118 , 31.577236 , 30.565132 , 30.82904  ,
          32.587406 , 32.69837  , 30.077812 , 33.508186 , 34.395542 ,
          34.74857  , 31.200544 , 32.430794 , 32.262714 , 33.08537  ,
          33.395508 , 29.128036 , 29.362097 , 29.608278 , 27.92922  ,
          27.02654  , 26.394283 , 28.339012 , 27.25325  , 27.914608 ,
          27.493322 , 28.818203 , 26.07845  , 26.297255 , 28.600729 ,
          27.653435 , 27.809496 , 28.893476 , 28.784111 , 28.29351  ,
          28.631182 , 28.20375  , 15.827183 , 20.83848  , 28.532713 ,
          26.407932 , 26.380068 , 25.573887 , 21.184254 , 26.372822 ,
          26.85384  , 27.186338 , 24.613485 , 24.034958 , 26.529345 ,
          27.095375 , 26.01566  , 28.533981 , 28.779293 , 26.78483  ,
          25.258823 , 24.121305 , 25.542557 , 26.027983 , 25.805328 ,
          26.068983 , 25.050343 , 25.963276 , 26.386597 , 26.41921  ,
          27.66286  , 28.730894 , 29.326147 , 29.848225 , 28.787792 ,
          29.89353  , 29.584927 , 30.182766 , 29.392458 , 29.99966  ,
          29.57572  , 28.808083 , 28.217983 , 27.245605 , 27.927732 ,
          28.180653 , 29.684679 , 30.347237 , 28.715103 , 29.052837 ,
          29.060444 , 29.475107 , 29.56176  , 30.340288 , 29.287489 ,
          29.029518 , 28.780752 , 28.22973  , 29.160313 , 29.684525 ,
          27.260353 , 26.054703 , 26.877941 , 28.052938 , 28.59358  ,
          26.314325 , 28.004002 , 28.309336 , 27.970922 , 24.17208  ,
          28.47196  , 28.379786 , 28.33188  , 29.347889 , 28.264673 ,
          28.490177 , 26.092579 , 28.270226 , 29.259161 , 30.01529  ,
          30.176893 , 30.951273 , 31.655579 , 31.866587 , 33.29403  ,
          32.32773  , 32.391518 , 33.013805 , 34.2049   , 35.39885  ,
          33.71306  , 34.747    , 34.328926 , 34.239143 , 32.38846  ,
          33.92605  , 32.34575  , 32.566185 , 30.322432 , 30.019318 ,
          31.097141 , 29.253351 , 31.422064 , 33.120487 , 33.34096  ,
          34.198757 , 33.978367 , 34.00793  , 34.345467 , 33.55096  ,
          33.821102 , 33.069893 , 32.86123  , 32.431793 , 32.791203 ,
          32.479095 , 33.151684 , 31.260265 , 30.307457 , 26.91224  ,
          22.465515 , 26.214489 , 18.628647 , 28.102814 , 28.87575  ,
          27.881306 , 30.460846 , 31.438694 , 30.465012 , 29.497032 ,
          30.0089   , 29.787655 , 28.00818  , 24.77789  , 27.852478 ,
          27.819096 , 30.92728  , 30.650131 , 30.579054 , 28.079836 ,
          31.493883 , 32.205677 , 29.632847 , 29.815783 , 30.475258 ,
          23.096104 , 22.133955 , 22.233164 , 30.209286 , 31.261034 ,
          26.111069 , 31.214556 , 29.260584 , 27.040693 , 30.788288 ,
          30.552942 , 30.781445 , 31.151209 , 28.85097  , 28.95206  ,
          30.680902 , 32.994823 , 33.09333  , 32.04963  , 32.48223  ,
          30.602373 , 30.763361 , 30.8481   , 31.841734 , 34.812283 ,
          30.905994 , 28.58185  , 28.962284 , 31.419739 , 31.667522 ,
          30.726204 , 30.979689 , 32.892284 , 35.24731  , 32.386612 ,
          32.536953 , 27.545795 , 30.979244 , 34.07229  , 29.211065 ,
          31.310766 , 27.148113 , 29.085176 , 29.240799 , 30.554121 ,
          30.748781 , 28.725815 , 28.75438  , 30.609417 , 32.094326 ,
          30.310951 , 31.347704 , 32.299927 , 31.04108  , 28.933968 ,
          29.49194  , 29.331661 , 22.787268 , 28.163752 , 27.508389 ,
          27.15601  , 30.905706 , 29.17673  , 28.989784 , 27.551468 ,
          27.799349 , 27.869843 , 30.540209 , 29.43601  , 31.478884 ,
          31.646564 , 31.34626  , 29.995522 , 30.587994 , 30.872017 ,
          33.53783  , 31.656082 , 31.184862 , 29.8596   , 31.200302 ,
          28.988365 , 31.1772   , 32.04352  , 33.33186  , 34.686695 ,
          32.270588 , 32.81985  , 34.82245  , 30.255907 , 27.000778 ,
          31.60639  , 33.21228  , 30.502766 , 29.421745 , 30.67129  ,
          33.128506 , 31.022188 , 32.806087 , 34.58911  , 34.167183 ,
          35.84772  , 36.8175   , 34.41109  , 34.334137 , 32.636158 ,
          33.382095 , 32.60263  , 32.761208 , 34.902267 , 33.970512 ,
          32.612194 , 35.29174  , 35.63135  , 37.379665 , 37.15835  ,
          37.996864 , 38.55955  , 39.62931  , 40.94131  , 39.045547 ,
          39.259083 , 40.67536  , 43.198765 , 41.556595 , 40.81526  ,
          40.75979  , 39.169403 , 40.062454 , 39.544426 , 43.903584 ,
          42.37286  , 42.325348 , 38.61     , 42.271305 , 40.341484 ,
          40.199306 , 40.85454  , 40.683975 , 41.333935 , 40.204926 ,
          41.462906 , 39.571663 , 41.945198 , 41.608612 , 42.790592 ,
          41.463665 , 45.25569  , 43.36491  , 41.163616 , 43.871017 ,
          45.4927   , 42.59713  , 41.977512 , 45.38557  , 44.078102 ,
          43.563152 , 43.628407 , 43.38679  , 43.35113  , 42.114872 ,
          41.94418  , 41.04163  , 42.020454 , 43.02959  , 41.9142   ,
          40.711254 , 42.9427   , 42.758297 , 42.442474 , 43.66887  ,
          43.3195   , 45.293713 , 42.29823  , 41.17005  , 38.19075  ,
          36.101604 , 38.252167 , 38.092922 , 37.095108 , 37.838417 ,
          37.761803 , 39.38203  , 39.604942 , 38.35947  , 37.192986 ,
          34.91715  , 34.0571   , 39.86787  , 38.606857 , 37.521458 ,
          29.980778 , 29.033047 , 27.31145  , 13.800947 ,  6.0904064,
           9.970101 , 30.535395 , 20.79597  , 20.864275 , 32.450157 ,
          33.952015 , 26.99879  , 30.519081 , 34.91247  , 33.947    ,
          17.917217 , 28.553782 , 37.87419  , 39.01217  , 39.743526 ,
          40.577442 , 40.99245  , 41.410915 , 42.328983 , 41.977943 ,
          46.016853 , 45.31137  , 48.043182 , 45.53443  , 44.655006 ,
          46.543743 , 48.812256 , 49.53628  , 50.94025  , 51.33296  ,
          48.679546 , 51.131973 , 49.402878 , 50.521526 , 51.516975 ,
          45.923653 , 50.18677  , 48.530865 , 49.261944 , 45.76501  ,
          46.07821  , 46.40535  , 42.76134  , 42.00964  , 44.957035 ,
          47.39794  , 47.693336 , 48.011383 , 49.165527 , 46.89182  ,
          49.04584  , 49.60355  , 51.375633 , 53.59237  , 54.29658  ,
          55.176777 , 54.874542 , 52.320477 , 52.85983  , 55.06989  ,
          54.72569  , 55.65646  , 56.23277  , 56.42413  , 57.19465  ,
          57.115982 , 58.316833 , 56.682034 , 56.355537 , 55.877407 ,
          57.45604  , 58.18929  , 57.33557  , 58.4829   , 58.71005  ,
          59.28448  , 59.773758 , 57.70367  , 56.553947 , 55.581455 ,
          58.48933  , 57.707726 , 57.668633 , 58.90658  , 58.638313 ,
          58.92934  , 54.6363   , 56.973274 , 58.90937  , 57.008236 ,
          55.968906 , 52.896873 , 54.284573 , 54.306793 , 54.622375 ,
          54.832138 , 55.11455  , 55.21072  , 53.679844 , 53.95129  ,
          48.443203 , 53.31453  , 53.152607 , 54.75939  , 53.162964 ,
          55.702606 , 54.479435 , 54.600582 , 55.40517  , 55.008972 ,
          61.249275 , 56.707996 , 58.786995 , 61.139122 , 59.757816 ,
          59.712875 , 59.467102 , 61.925026 , 62.40662  , 60.2509   ,
          58.961067 , 58.977623 , 59.357323 , 60.032764 , 56.529358 ,
          55.562675 , 59.210716 , 56.242374 , 54.238422 , 53.827488 ,
          53.938107 , 49.232834 , 43.94818  , 47.780464 ,  7.518135 ,
          22.608507 , 34.081154 , 40.401752 , 37.462734 , 21.43678  ,
          31.341328 , 43.646458 , 22.532    , 44.00136  , 44.447636 ,
          44.912804 , 48.062458 , 46.74743  , 49.90155  , 49.416477 ,
          51.37282  , 48.461166 , 45.72746  , 46.469196 , 46.934605 ,
          44.053886 , 22.644484 ,  6.8766646, 42.375244 , 44.543037 ,
          44.539543 , 42.44923  , 43.2529   , 43.820484 , 39.382084 ,
          37.480038 , 32.197952 , 44.780067 , 48.77445  , 46.61584  ,
          46.138626 , 43.456142 , 42.042835 , 43.18084  , 44.793903 ,
          46.872726 , 47.292267 , 45.51595  , 46.30709  , 47.181713 ,
          48.00544  , 47.559364 , 48.243324 , 46.930027 , 43.474533 ,
          43.177113 , 46.444042 , 48.409523 , 49.503876 , 48.877235 ,
          48.031235 , 49.474457 , 49.27266  , 47.87698  , 48.969242 ,
          52.262043 , 54.22861  , 51.19327  , 52.31781  , 52.05568  ,
          54.560577 , 54.59873  , 45.36566  , 47.72178  , 47.66583  ,
          49.503857 , 48.44381  , 48.557514 , 49.059517 , 51.484367 ,
          51.29379  , 47.785267 , 47.74033  , 48.47073  , 49.527966 ,
          53.69879  , 53.383175 , 52.97548  , 55.211697 , 48.94556  ,
          54.80213  , 57.946636 , 52.23061  , 55.074226 , 52.95395  ,
          54.286552 , 56.05054  , 57.66334  ], dtype=float32),
   &#39;run_time&#39;: 2.62833571434021,
   &#39;status&#39;: &#39;ok&#39;},
  &#39;misc&#39;: {&#39;tid&#39;: 0,
   &#39;cmd&#39;: (&#39;domain_attachment&#39;, &#39;FMinIter_Domain&#39;),
   &#39;workdir&#39;: None,
   &#39;idxs&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_inputs&#39;: [0],
    &#39;complete_sample&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0],
    &#39;dropout_prob_theta&#39;: [0],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [0],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0],
    &#39;len_sample_chunks&#39;: [0],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_harmonics&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_polynomials&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_series_per_batch&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_val_weeks&#39;: [0],
    &#39;n_x_hidden&#39;: [0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [0],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [0],
    &#39;window_sampling_limit&#39;: [0]},
   &#39;vals&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_inputs&#39;: [0],
    &#39;complete_sample&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0.24642550819005665],
    &#39;dropout_prob_theta&#39;: [0.04453225609606154],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [1],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0.0005903125740803596],
    &#39;len_sample_chunks&#39;: [0],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0.47290326767085267],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_harmonics&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_polynomials&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_series_per_batch&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_val_weeks&#39;: [0],
    &#39;n_x_hidden&#39;: [5.0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [16.0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [2],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [5.8073579520205266e-05],
    &#39;window_sampling_limit&#39;: [0]}},
  &#39;exp_key&#39;: None,
  &#39;owner&#39;: None,
  &#39;version&#39;: 0,
  &#39;book_time&#39;: datetime.datetime(2021, 6, 16, 16, 19, 26, 482000),
  &#39;refresh_time&#39;: datetime.datetime(2021, 6, 16, 16, 19, 29, 129000)},
 {&#39;state&#39;: 2,
  &#39;tid&#39;: 1,
  &#39;spec&#39;: None,
  &#39;result&#39;: {&#39;loss&#39;: 1.9704331159591675,
   &#39;mc&#39;: {&#39;activation&#39;: &#39;SELU&#39;,
    &#39;batch_normalization&#39;: False,
    &#39;batch_size&#39;: 256,
    &#39;complete_inputs&#39;: False,
    &#39;complete_sample&#39;: False,
    &#39;device&#39;: &#39;cuda&#39;,
    &#39;dropout_prob_exogenous&#39;: 0.36553112690476813,
    &#39;dropout_prob_theta&#39;: 0.24107333575459605,
    &#39;early_stop_patience&#39;: 16,
    &#39;eval_freq&#39;: 50,
    &#39;frequency&#39;: &#39;H&#39;,
    &#39;idx_to_sample_freq&#39;: 24,
    &#39;initialization&#39;: &#39;he_normal&#39;,
    &#39;l1_theta&#39;: 0,
    &#39;learning_rate&#39;: 0.0008532112183090035,
    &#39;len_sample_chunks&#39;: None,
    &#39;loss_hypar&#39;: 0.5,
    &#39;loss_train&#39;: &#39;MAE&#39;,
    &#39;loss_valid&#39;: &#39;MAE&#39;,
    &#39;lr_decay&#39;: 0.3553227609462469,
    &#39;lr_decay_step_size&#39;: 100,
    &#39;max_epochs&#39;: 10,
    &#39;max_steps&#39;: None,
    &#39;mode&#39;: &#39;simple&#39;,
    &#39;model&#39;: &#39;nbeats&#39;,
    &#39;n_blocks&#39;: (1, 1),
    &#39;n_harmonics&#39;: 1,
    &#39;n_hidden&#39;: 256,
    &#39;n_layers&#39;: (2, 2),
    &#39;n_polynomials&#39;: 2,
    &#39;n_s_hidden&#39;: 0,
    &#39;n_series_per_batch&#39;: 1,
    &#39;n_time_in&#39;: 168,
    &#39;n_time_out&#39;: 24,
    &#39;n_val_weeks&#39;: 104,
    &#39;n_x_hidden&#39;: 6.0,
    &#39;normalizer_x&#39;: &#39;median&#39;,
    &#39;normalizer_y&#39;: None,
    &#39;random_seed&#39;: 18.0,
    &#39;seasonality&#39;: 24,
    &#39;shared_weights&#39;: False,
    &#39;stack_types&#39;: (&#39;identity&#39;, &#39;identity&#39;),
    &#39;val_idx_to_sample_freq&#39;: 24,
    &#39;weight_decay&#39;: 0.00012349475141123045,
    &#39;window_sampling_limit&#39;: 100000,
    &#39;n_x&#39;: 1,
    &#39;n_s&#39;: 1,
    &#39;n_theta_hidden&#39;: [[256, 256], [256, 256]]},
   &#39;y_true&#39;: array([25.73, 29.37, 28.76, 25.95, 26.71, 29.36, 30.93, 28.3 , 30.58,
          33.32, 28.86, 30.5 , 29.72, 28.69, 28.  , 25.04, 28.72, 29.79,
          29.41, 30.89, 30.99, 29.56, 27.84, 28.19, 27.36, 27.84, 28.13,
          28.42, 28.4 , 27.41, 28.66, 28.67, 28.51, 27.96, 29.31, 29.99,
          28.97, 29.96, 30.59, 30.8 , 30.62, 31.01, 32.  , 33.14, 32.63,
          32.11, 31.77, 31.04, 31.35, 31.96, 31.01, 29.98, 30.36, 28.15,
          28.87, 27.66, 29.85, 26.38, 27.82, 30.46, 28.49, 28.67, 27.58,
          29.55, 29.93, 29.03, 30.83, 29.08, 30.54, 30.97, 32.54, 30.1 ,
          29.72, 31.56, 29.66, 29.13, 29.94, 25.2 , 28.56, 27.86, 25.65,
          29.17, 27.05, 27.23, 28.04, 28.94, 29.22, 28.55, 28.05, 28.15,
          28.5 , 28.9 , 29.68, 27.87, 28.14, 27.49, 27.38, 28.06, 27.12,
          24.5 , 26.57, 27.03, 27.59, 25.54, 22.99, 25.42, 24.72, 27.11,
          28.73, 27.15, 29.23, 28.95, 29.76, 29.91, 27.27, 24.44, 25.9 ,
          29.06, 28.89, 31.36, 31.24, 31.39, 30.1 , 30.12, 27.03, 27.1 ,
          28.  , 27.99, 27.48, 29.84, 30.25, 27.39, 30.99, 31.73, 32.4 ,
          28.76, 30.05, 30.05, 30.26, 30.21, 26.95, 26.68, 26.74, 25.06,
          25.17, 24.04, 25.8 , 25.6 , 25.65, 25.42, 26.33, 24.14, 24.06,
          26.78, 26.11, 25.61, 26.57, 26.97, 26.21, 26.29, 26.2 , 13.97,
          19.62, 26.43, 24.68, 24.5 , 24.28, 20.64, 25.43, 24.76, 24.89,
          22.06, 23.07, 24.79, 24.81, 24.03, 26.5 , 26.34, 24.42, 23.49,
          21.8 , 23.48, 24.23, 23.69, 23.57, 22.99, 24.14, 24.19, 24.33,
          25.6 , 26.23, 26.55, 27.41, 26.73, 27.4 , 27.12, 27.44, 26.51,
          26.96, 26.51, 26.33, 25.51, 25.02, 25.63, 25.47, 26.65, 27.45,
          26.47, 26.65, 26.77, 27.02, 26.89, 27.46, 26.35, 26.75, 26.36,
          26.42, 26.7 , 27.04, 25.04, 23.65, 24.86, 25.4 , 26.11, 24.57,
          25.86, 25.87, 25.57, 21.97, 26.01, 26.3 , 26.17, 26.85, 25.74,
          25.99, 23.77, 25.79, 27.04, 27.77, 27.7 , 28.65, 29.16, 29.65,
          30.36, 29.78, 30.32, 30.15, 31.4 , 32.27, 31.37, 31.55, 31.47,
          31.56, 29.66, 30.83, 29.27, 29.87, 27.35, 27.67, 28.47, 26.5 ,
          28.19, 30.22, 31.05, 31.52, 31.57, 31.55, 31.55, 30.51, 30.65,
          30.42, 29.68, 29.51, 29.8 , 29.38, 29.81, 27.96, 27.85, 24.06,
          20.41, 24.3 , 17.11, 26.12, 26.91, 26.6 , 28.79, 30.26, 28.56,
          27.11, 27.63, 26.63, 26.03, 21.72, 26.06, 25.26, 28.37, 27.81,
          27.91, 26.  , 28.97, 29.57, 27.19, 27.1 , 27.64, 20.1 , 20.06,
          20.03, 28.5 , 28.95, 23.95, 28.85, 27.1 , 24.96, 28.03, 28.25,
          28.3 , 28.57, 26.03, 26.01, 28.29, 29.71, 30.37, 29.74, 29.66,
          27.57, 27.58, 27.88, 28.67, 31.55, 29.76, 27.01, 26.05, 29.13,
          28.24, 28.01, 28.62, 30.82, 32.18, 30.92, 30.97, 27.9 , 28.24,
          29.83, 27.07, 27.94, 24.64, 26.06, 26.79, 27.52, 28.21, 27.96,
          26.49, 28.26, 29.03, 29.02, 28.59, 30.43, 29.06, 28.  , 27.3 ,
          26.94, 20.08, 25.65, 25.22, 24.51, 28.22, 26.82, 26.68, 25.59,
          25.43, 25.02, 27.55, 27.31, 28.07, 29.66, 29.03, 27.75, 28.11,
          28.6 , 30.37, 30.05, 29.53, 28.79, 28.82, 25.69, 28.05, 29.31,
          30.45, 32.79, 31.64, 30.52, 33.06, 28.08, 25.5 , 28.98, 30.08,
          28.01, 27.57, 28.33, 30.14, 27.7 , 29.53, 31.9 , 32.12, 32.78,
          34.9 , 33.7 , 32.74, 30.91, 31.17, 29.96, 28.87, 31.02, 31.14,
          29.74, 32.22, 32.97, 34.89, 33.77, 35.77, 37.41, 37.34, 38.21,
          36.05, 36.93, 36.48, 38.98, 37.72, 37.73, 39.09, 39.81, 38.41,
          38.17, 40.4 , 38.6 , 37.29, 35.57, 37.9 , 36.03, 35.68, 36.62,
          37.64, 37.75, 37.68, 37.43, 36.79, 37.14, 37.48, 39.6 , 37.88,
          41.43, 39.59, 38.68, 39.54, 42.1 , 39.38, 39.26, 40.87, 39.52,
          39.56, 39.22, 39.22, 39.13, 38.39, 38.09, 37.42, 38.62, 38.68,
          38.01, 37.64, 38.96, 38.5 , 37.96, 40.01, 39.18, 41.55, 39.83,
          38.04, 34.93, 32.64, 35.34, 33.97, 34.21, 34.77, 34.35, 35.93,
          36.02, 35.91, 33.77, 32.13, 31.09, 36.03, 35.26, 34.15, 28.06,
          27.2 , 24.88, 12.42,  4.44,  9.48, 30.11, 19.5 , 20.07, 32.98,
          32.97, 24.98, 29.06, 33.81, 31.77, 15.27, 28.05, 36.81, 36.07,
          36.34, 39.04, 39.12, 37.69, 39.17, 38.85, 42.26, 41.36, 44.16,
          42.2 , 40.66, 43.07, 44.91, 45.17, 46.5 , 47.28, 45.29, 46.73,
          45.1 , 46.11, 46.91, 41.1 , 45.56, 44.46, 44.48, 41.49, 42.22,
          41.97, 38.05, 37.47, 41.01, 43.27, 43.92, 44.45, 45.29, 42.82,
          44.73, 45.48, 46.78, 48.94, 50.24, 50.84, 50.19, 47.56, 48.25,
          49.76, 49.68, 50.8 , 51.54, 51.2 , 51.94, 52.4 , 53.18, 51.73,
          51.33, 50.51, 51.9 , 52.56, 52.44, 53.29, 53.92, 54.39, 54.33,
          51.99, 50.87, 50.53, 53.  , 52.67, 52.72, 53.6 , 53.33, 53.44,
          49.6 , 51.51, 53.6 , 51.78, 50.74, 47.48, 49.08, 49.32, 49.43,
          49.87, 50.53, 50.59, 48.72, 48.83, 43.64, 48.23, 48.17, 49.92,
          48.26, 50.62, 49.63, 49.77, 50.14, 50.15, 55.8 , 52.37, 54.05,
          56.58, 55.2 , 53.87, 53.8 , 56.61, 56.84, 54.61, 53.44, 53.65,
          53.05, 54.19, 50.99, 49.92, 53.13, 50.19, 49.35, 48.58, 48.91,
          44.41, 39.04, 43.01,  3.27, 21.04, 30.61, 39.25, 34.23, 20.99,
          30.48, 41.79, 19.92, 42.05, 42.76, 42.51, 44.67, 43.34, 45.61,
          45.34, 47.03, 44.08, 41.38, 41.88, 42.4 , 39.36, 19.55,  4.91,
          40.59, 41.44, 41.01, 39.89, 41.37, 40.77, 35.08, 34.58, 30.02,
          41.19, 44.3 , 42.5 , 44.23, 40.99, 39.09, 39.04, 40.23, 41.93,
          42.54, 41.44, 42.1 , 42.99, 43.91, 43.49, 43.8 , 42.69, 39.58,
          38.47, 41.95, 43.68, 45.17, 44.97, 43.78, 45.38, 44.16, 43.48,
          44.65, 47.6 , 49.43, 47.59, 48.16, 47.88, 49.28, 49.5 , 42.4 ,
          42.66, 43.21, 43.23, 42.89, 44.65, 45.06, 46.69, 46.56, 43.81,
          44.25, 43.8 , 44.43, 48.58, 49.21, 49.65, 51.36, 46.47, 49.86,
          52.49, 48.69, 50.12, 48.12, 49.01, 50.47, 52.32, 48.1 ],
         dtype=float32),
   &#39;y_hat&#39;: array([26.316542 , 26.332678 , 29.868633 , 29.212381 , 26.364874 ,
          26.917328 , 29.745722 , 31.536858 , 28.970425 , 31.199947 ,
          33.604595 , 29.44432  , 31.221601 , 30.21905  , 29.22446  ,
          28.915838 , 25.89044  , 29.230467 , 30.18915  , 29.975319 ,
          31.366655 , 31.25101  , 30.071215 , 28.731358 , 28.535212 ,
          27.618187 , 29.029678 , 29.363562 , 29.049566 , 28.913328 ,
          28.011992 , 29.117641 , 29.140635 , 29.044228 , 28.66233  ,
          29.913248 , 30.595543 , 29.616873 , 30.44652  , 31.032312 ,
          31.402649 , 31.28904  , 31.644335 , 32.50862  , 33.501453 ,
          32.649067 , 32.44691  , 32.40678  , 31.825151 , 32.10069  ,
          33.180458 , 31.670181 , 30.474667 , 30.778875 , 28.836325 ,
          29.560535 , 28.58201  , 30.497644 , 26.998016 , 28.269503 ,
          30.825726 , 28.996218 , 29.206642 , 28.11583  , 30.245121 ,
          30.50653  , 29.545559 , 31.27154  , 29.62038  , 31.091951 ,
          31.401278 , 32.87783  , 30.627516 , 30.204292 , 32.011948 ,
          30.288706 , 30.168816 , 30.597305 , 25.951916 , 29.145744 ,
          28.223104 , 26.144302 , 29.699013 , 27.67291  , 27.693502 ,
          28.665813 , 29.434685 , 29.595596 , 28.981207 , 28.706457 ,
          28.617367 , 29.054865 , 29.54745  , 30.282429 , 28.1299   ,
          28.439993 , 27.934156 , 28.018017 , 28.524834 , 27.756063 ,
          25.052336 , 26.91068  , 27.404531 , 28.184435 , 26.043272 ,
          23.491674 , 25.962397 , 25.21885  , 27.582739 , 29.087084 ,
          27.533033 , 29.615124 , 29.390865 , 30.074804 , 30.195353 ,
          27.71758  , 24.908722 , 26.24871  , 29.815231 , 29.513865 ,
          31.788504 , 31.33913  , 31.55614  , 30.190062 , 30.616823 ,
          27.675676 , 27.844856 , 28.76709  , 28.721764 , 27.732212 ,
          30.144617 , 30.558546 , 27.959848 , 31.511566 , 32.05157  ,
          32.824352 , 28.970306 , 30.320168 , 30.302729 , 31.071394 ,
          30.95294  , 27.713512 , 27.281979 , 27.102125 , 25.239094 ,
          25.68945  , 24.52259  , 26.123549 , 26.075962 , 26.127329 ,
          25.531578 , 26.201128 , 24.493105 , 24.480654 , 26.935532 ,
          26.475863 , 25.935757 , 26.59186  , 26.857464 , 26.650738 ,
          26.893343 , 26.680841 , 14.411107 , 19.875776 , 26.351929 ,
          24.763182 , 24.917603 , 24.733768 , 20.66013  , 25.580776 ,
          24.815365 , 25.177776 , 22.208311 , 23.26691  , 24.852436 ,
          25.302544 , 24.503868 , 26.88481  , 26.467033 , 24.698137 ,
          23.865673 , 22.090553 , 23.847254 , 24.60143  , 24.05421  ,
          23.585033 , 23.06441  , 24.394993 , 24.594437 , 24.765886 ,
          26.094141 , 26.627785 , 26.85206  , 27.626198 , 27.08107  ,
          27.824722 , 27.614399 , 27.995546 , 27.070292 , 27.359705 ,
          26.868534 , 26.771917 , 26.021866 , 25.52235  , 26.10642  ,
          25.980162 , 27.049232 , 27.784496 , 26.839733 , 27.043272 ,
          27.195377 , 27.59984  , 27.399073 , 27.767735 , 26.6787   ,
          27.154633 , 26.630852 , 26.905252 , 27.053562 , 27.463858 ,
          25.143562 , 23.751587 , 25.07697  , 25.762617 , 26.608791 ,
          25.115149 , 26.088032 , 25.91419  , 25.679714 , 22.293678 ,
          26.299696 , 26.720356 , 26.651005 , 27.149632 , 25.823586 ,
          26.134111 , 24.134195 , 26.175089 , 27.44743  , 28.230444 ,
          28.000671 , 28.873348 , 29.42867  , 30.11124  , 30.919012 ,
          30.2908   , 30.877293 , 30.623606 , 31.723726 , 32.62584  ,
          32.156883 , 32.145863 , 32.08568  , 32.12507  , 30.233706 ,
          31.256123 , 29.642857 , 30.489922 , 27.976486 , 28.370348 ,
          29.065554 , 27.136566 , 28.586424 , 30.631649 , 31.50462  ,
          32.025738 , 32.114616 , 32.210636 , 32.140903 , 30.921831 ,
          31.137455 , 31.053627 , 30.380733 , 30.206448 , 30.584906 ,
          30.054352 , 30.343832 , 28.48287  , 28.451479 , 24.675198 ,
          21.146889 , 24.696201 , 17.433626 , 26.086557 , 27.301338 ,
          27.265387 , 29.282425 , 30.623447 , 29.146275 , 27.52272  ,
          27.98941  , 26.849022 , 26.740168 , 22.38552  , 26.839417 ,
          25.774244 , 29.045444 , 27.960386 , 28.351294 , 26.508701 ,
          29.440002 , 30.05442  , 27.924397 , 27.73619  , 27.995207 ,
          20.528921 , 20.765812 , 20.544168 , 28.832373 , 29.41216  ,
          24.71784  , 29.136946 , 27.31434  , 25.516087 , 28.598337 ,
          28.874022 , 28.856686 , 29.12458  , 26.513725 , 26.403013 ,
          28.738852 , 30.380236 , 31.10622  , 30.421041 , 30.409613 ,
          27.905996 , 28.16787  , 28.562185 , 29.52323  , 32.14226  ,
          30.318306 , 28.027819 , 26.413975 , 29.565416 , 28.970282 ,
          29.826061 , 29.512224 , 31.537811 , 32.631557 , 31.484304 ,
          31.545141 , 28.204159 , 28.776415 , 31.095438 , 29.054096 ,
          29.708689 , 25.235985 , 26.537136 , 27.516718 , 28.378603 ,
          28.342533 , 28.741978 , 27.157059 , 28.862612 , 29.119162 ,
          30.469688 , 29.117876 , 30.994406 , 29.255133 , 29.341637 ,
          27.652409 , 27.545116 , 20.736565 , 27.372335 , 26.40428  ,
          25.29222  , 28.8366   , 27.054873 , 27.304193 , 25.882988 ,
          25.884678 , 25.54018  , 28.416327 , 28.001759 , 28.558659 ,
          29.993645 , 29.379341 , 28.49898  , 28.465815 , 29.212906 ,
          30.855318 , 30.34043  , 29.961828 , 29.639576 , 29.473892 ,
          26.471424 , 29.210135 , 30.053345 , 30.727957 , 32.94761  ,
          32.15546  , 31.193436 , 33.843246 , 28.561438 , 26.848959 ,
          29.268343 , 30.703053 , 28.9061   , 29.35433  , 28.848923 ,
          30.778301 , 28.436895 , 30.09379  , 31.942581 , 32.588036 ,
          33.461864 , 34.643566 , 33.95859  , 33.534912 , 31.044155 ,
          31.758682 , 31.487186 , 29.986835 , 32.09246  , 32.108078 ,
          30.08011  , 32.725605 , 33.202538 , 35.461895 , 34.702103 ,
          36.060993 , 37.93341  , 37.990437 , 38.69841  , 36.546772 ,
          38.402966 , 37.67462  , 40.327503 , 38.808056 , 38.59757  ,
          37.354355 , 40.097652 , 39.22136  , 40.11081  , 42.41455  ,
          41.79919  , 38.72615  , 36.235508 , 38.690426 , 37.238087 ,
          36.860565 , 37.63107  , 38.600697 , 38.1995   , 38.342266 ,
          38.040173 , 38.063576 , 38.05651  , 38.64399  , 40.61821  ,
          38.614876 , 42.18752  , 39.919403 , 39.836254 , 40.241505 ,
          42.736107 , 39.877953 , 40.51393  , 41.508724 , 40.397    ,
          40.910168 , 40.498013 , 40.27202  , 39.93956  , 39.096313 ,
          38.720898 , 38.018013 , 39.356915 , 39.80988  , 39.0107   ,
          38.62855  , 39.78748  , 39.326687 , 38.84446  , 41.0381   ,
          40.05979  , 42.211327 , 40.55543  , 38.72581  , 35.50433  ,
          33.15156  , 36.467445 , 34.798786 , 35.143898 , 35.689423 ,
          35.015102 , 36.370106 , 36.476543 , 36.59173  , 34.445282 ,
          33.071697 , 31.722181 , 36.62848  , 35.782845 , 34.39219  ,
          28.274609 , 27.277254 , 25.39128  , 12.862366 ,  4.698904 ,
           9.863125 , 29.90889  , 19.838423 , 21.137693 , 33.227917 ,
          32.970398 , 26.007902 , 29.414219 , 33.386658 , 32.39855  ,
          16.183578 , 28.62634  , 36.90141  , 36.811344 , 36.869556 ,
          39.44994  , 39.804955 , 38.48359  , 40.009052 , 39.709885 ,
          43.02932  , 41.918503 , 44.804497 , 43.032177 , 41.58474  ,
          43.969055 , 45.815964 , 46.111465 , 47.253162 , 48.00756  ,
          46.111145 , 47.686134 , 46.296547 , 47.267548 , 47.958515 ,
          42.02845  , 46.445835 , 45.28598  , 45.598274 , 42.70627  ,
          43.30676  , 42.91156  , 38.901825 , 38.331062 , 41.85373  ,
          44.1439   , 44.823734 , 45.402973 , 46.05648  , 43.599392 ,
          45.608562 , 46.367874 , 47.931347 , 50.099613 , 51.423164 ,
          51.921333 , 51.116806 , 48.533573 , 49.353764 , 51.01707  ,
          50.999355 , 52.101753 , 52.731388 , 52.267128 , 53.069786 ,
          53.526093 , 54.40946  , 52.984356 , 52.713955 , 51.782814 ,
          52.99036  , 53.61709  , 53.616722 , 54.582867 , 55.174137 ,
          55.692905 , 55.687843 , 53.244335 , 52.042747 , 51.7111   ,
          54.247387 , 53.975327 , 54.110504 , 54.86047  , 54.442204 ,
          54.59453  , 50.847523 , 52.842854 , 54.923733 , 53.203354 ,
          52.044605 , 48.57166  , 50.018295 , 50.36679  , 50.743805 ,
          51.15432  , 51.724545 , 51.65057  , 49.591064 , 49.721752 ,
          44.74381  , 49.513412 , 49.29022  , 51.188946 , 49.48531  ,
          51.719593 , 50.495228 , 50.902714 , 51.452557 , 51.513752 ,
          56.99558  , 53.626606 , 55.109184 , 57.32006  , 56.420982 ,
          55.64347  , 55.371906 , 58.149998 , 58.330627 , 55.8005   ,
          54.714012 , 55.281902 , 54.717293 , 55.871803 , 52.709415 ,
          51.4374   , 54.2903   , 51.54014  , 50.73443  , 49.893337 ,
          50.198578 , 45.737324 , 40.384453 , 43.900368 ,  4.19298  ,
          21.630276 , 31.28498  , 40.02079  , 34.719013 , 22.665482 ,
          30.285952 , 42.299335 , 20.716028 , 42.98018  , 43.155254 ,
          43.601974 , 45.674557 , 44.030983 , 46.29506  , 46.5326   ,
          48.242615 , 45.327084 , 42.744022 , 42.876106 , 43.25196  ,
          40.296913 , 20.625963 ,  5.4051633, 40.82934  , 41.9999   ,
          42.35335  , 41.01998  , 41.718838 , 41.781094 , 36.02886  ,
          35.426888 , 30.570454 , 41.61625  , 45.09385  , 43.553562 ,
          45.386765 , 41.580982 , 40.024704 , 40.175323 , 41.487953 ,
          42.785683 , 43.610043 , 42.59862  , 43.141163 , 44.079082 ,
          45.06972  , 44.559586 , 44.769234 , 43.62031  , 40.80087  ,
          39.618958 , 43.246197 , 44.907097 , 46.330902 , 45.865074 ,
          44.80873  , 46.673355 , 45.473827 , 44.896606 , 45.874157 ,
          48.74119  , 50.453503 , 48.681767 , 49.592327 , 48.882355 ,
          50.794422 , 50.980145 , 44.191353 , 43.60242  , 44.356518 ,
          44.391727 , 45.00895  , 46.265785 , 46.24252  , 47.741894 ,
          47.641434 , 44.845787 , 45.413986 , 44.862644 , 45.823215 ,
          50.0524   , 50.28477  , 50.6477   , 52.38364  , 47.75348  ,
          51.431583 , 54.140472 , 50.58671  , 52.07413  , 49.381317 ,
          50.11864  , 52.503525 , 54.325073 ], dtype=float32),
   &#39;run_time&#39;: 2.133913516998291,
   &#39;status&#39;: &#39;ok&#39;},
  &#39;misc&#39;: {&#39;tid&#39;: 1,
   &#39;cmd&#39;: (&#39;domain_attachment&#39;, &#39;FMinIter_Domain&#39;),
   &#39;workdir&#39;: None,
   &#39;idxs&#39;: {&#39;activation&#39;: [1],
    &#39;batch_normalization&#39;: [1],
    &#39;batch_size&#39;: [1],
    &#39;complete_inputs&#39;: [1],
    &#39;complete_sample&#39;: [1],
    &#39;device&#39;: [1],
    &#39;dropout_prob_exogenous&#39;: [1],
    &#39;dropout_prob_theta&#39;: [1],
    &#39;early_stop_patience&#39;: [1],
    &#39;eval_freq&#39;: [1],
    &#39;frequency&#39;: [1],
    &#39;idx_to_sample_freq&#39;: [1],
    &#39;initialization&#39;: [1],
    &#39;l1_theta&#39;: [1],
    &#39;learning_rate&#39;: [1],
    &#39;len_sample_chunks&#39;: [1],
    &#39;loss&#39;: [1],
    &#39;loss_hypar&#39;: [1],
    &#39;loss_valid&#39;: [1],
    &#39;lr_decay&#39;: [1],
    &#39;lr_decay_step_size&#39;: [1],
    &#39;max_epochs&#39;: [1],
    &#39;max_steps&#39;: [1],
    &#39;n_blocks&#39;: [1],
    &#39;n_harmonics&#39;: [1],
    &#39;n_hidden&#39;: [1],
    &#39;n_layers&#39;: [1],
    &#39;n_polynomials&#39;: [1],
    &#39;n_s_hidden&#39;: [1],
    &#39;n_series_per_batch&#39;: [1],
    &#39;n_time_in&#39;: [1],
    &#39;n_time_out&#39;: [1],
    &#39;n_val_weeks&#39;: [1],
    &#39;n_x_hidden&#39;: [1],
    &#39;normalizer_x&#39;: [1],
    &#39;normalizer_y&#39;: [1],
    &#39;random_seed&#39;: [1],
    &#39;seasonality&#39;: [1],
    &#39;shared_weights&#39;: [1],
    &#39;stack_types&#39;: [1],
    &#39;val_idx_to_sample_freq&#39;: [1],
    &#39;weight_decay&#39;: [1],
    &#39;window_sampling_limit&#39;: [1]},
   &#39;vals&#39;: {&#39;activation&#39;: [0],
    &#39;batch_normalization&#39;: [0],
    &#39;batch_size&#39;: [0],
    &#39;complete_inputs&#39;: [0],
    &#39;complete_sample&#39;: [0],
    &#39;device&#39;: [0],
    &#39;dropout_prob_exogenous&#39;: [0.36553112690476813],
    &#39;dropout_prob_theta&#39;: [0.24107333575459605],
    &#39;early_stop_patience&#39;: [0],
    &#39;eval_freq&#39;: [0],
    &#39;frequency&#39;: [0],
    &#39;idx_to_sample_freq&#39;: [0],
    &#39;initialization&#39;: [1],
    &#39;l1_theta&#39;: [0],
    &#39;learning_rate&#39;: [0.0008532112183090035],
    &#39;len_sample_chunks&#39;: [0],
    &#39;loss&#39;: [0],
    &#39;loss_hypar&#39;: [0],
    &#39;loss_valid&#39;: [0],
    &#39;lr_decay&#39;: [0.3553227609462469],
    &#39;lr_decay_step_size&#39;: [0],
    &#39;max_epochs&#39;: [0],
    &#39;max_steps&#39;: [0],
    &#39;n_blocks&#39;: [0],
    &#39;n_harmonics&#39;: [0],
    &#39;n_hidden&#39;: [0],
    &#39;n_layers&#39;: [0],
    &#39;n_polynomials&#39;: [0],
    &#39;n_s_hidden&#39;: [0],
    &#39;n_series_per_batch&#39;: [0],
    &#39;n_time_in&#39;: [0],
    &#39;n_time_out&#39;: [0],
    &#39;n_val_weeks&#39;: [0],
    &#39;n_x_hidden&#39;: [6.0],
    &#39;normalizer_x&#39;: [0],
    &#39;normalizer_y&#39;: [0],
    &#39;random_seed&#39;: [18.0],
    &#39;seasonality&#39;: [0],
    &#39;shared_weights&#39;: [0],
    &#39;stack_types&#39;: [0],
    &#39;val_idx_to_sample_freq&#39;: [0],
    &#39;weight_decay&#39;: [0.00012349475141123045],
    &#39;window_sampling_limit&#39;: [0]}},
  &#39;exp_key&#39;: None,
  &#39;owner&#39;: None,
  &#39;version&#39;: 0,
  &#39;book_time&#39;: datetime.datetime(2021, 6, 16, 16, 19, 29, 151000),
  &#39;refresh_time&#39;: datetime.datetime(2021, 6, 16, 16, 19, 31, 305000)}]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

